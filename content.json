{"posts":[{"title":"利用 qmake 构建 Qt 项目与开发过程细节说明","text":"本文介绍使用 qmake 构建 Qt 应用程序的标准流程，并附带一个 .pro 文件的示例。同时说明了开发过程中在逻辑设计上的注意事项。 1. 利用 qmake 进行 Qt 项目编译Qt 提供的 qmake 工具可以自动根据 .pro 文件生成 Makefile 文件，从而简化项目的构建流程。 构建步骤：1234qmake # 生成 Makefilemake # 执行编译./MyApplication # 运行程序 .pro文件配置如下#————————————————-# Project created by QtCreator#————————————————- 指定项目模板类型，可为 app（应用程序）、lib（库）等TEMPLATE = app 生成的目标可执行文件名称TARGET = MyApplication 使用的 Qt 模块QT += core gui QT += widgets # 如使用 QWidget 可启用编译器配置CONFIG += c++17 console 源文件列表SOURCES += main.cpp mainwindow.cpp 头文件列表HEADERS += mainwindow.h UI 界面文件（来自 Qt Designer）FORMS += mainwindow.ui 资源文件（可选）RESOURCES += resources.qrc外部包含路径与库INCLUDEPATH += /path/to/includesLIBS += -L/path/to/libs -lname 编译宏定义DEFINES += QT_DEPRECATED_WARNINGS 输出目录与中间目录配置DESTDIR = binOBJECTS_DIR = build/objMOC_DIR = build/mocRCC_DIR = build/rccUI_DIR = build/ui 完整示例：12345678910111213141516171819202122232425262728293031323334353637DESTDIR = ./build/binOBJECTS_DIR = ./build/objMOC_DIR = ./build/mocRCC_DIR = ./build/rccUI_DIR = ./build/uiQT += core gui concurrentgreaterThan(QT_MAJOR_VERSION, 4): QT += widgets printsupportgreaterThan(QT_MAJOR_VERSION, 4): CONFIG += c++11lessThan(QT_MAJOR_VERSION, 5): QMAKE_CXXFLAGS += -std=c++11LIBS += -L/lib/aarch64-linux-gnu -lcurlLIBS += -L/lib/aarch64-linux-gnu -lpaho-mqtt3cLIBS += -L/lib/aarch64-linux-gnu -lqrencodeLIBS += -L/lib/aarch64-linux-gnu -lpngTARGET = collectTEMPLATE = appSOURCES += main.cpp\\ mainwindow.cpp\\ qcustomplot.cpp\\ max30102.cpp\\ MaxPlot.cpp \\ MQTTWorker.cpp\\ QRCodeGenerator.cpp HEADERS += qcustomplot.h\\ mainwindow.h\\ MaxPlot.h\\ max30102.h \\ MQTTWorker.h \\ QRCodeGenerator.h 开发过程中的细节考虑：在Qt的mainwindow.cpp文件里面，不可出现扫描通道的代码，原因是，在用户层面不需要考虑扫描通道的问题，只需要得到数据。","link":"/2024/12/05/Linux%E4%B8%8B%E7%9A%84QT%E8%AE%BE%E8%AE%A1/"},{"title":"关于个人发展的规划","text":"本文主要是做一个自我规划 最终目标：拿到大厂的ssp的c++开发岗位或者算法岗的薪资大方向的计划：1、提升c++代码撰写能力，找c++项目来写，由简到难2、回顾之前学的ACM算法，LeetCode多刷，每天保持刷题3、找实习，从日常实习开始，到大厂实习4、八股文准备（不急） 关于对C++的精进1、精进C++本身，包括STL库以及源码解析，精进进程，线程，多线程，线程池以及之间的调度2、精进Linux系统的使用，Linux多线程服务编程以及MySQL数据库的使用3、精进cuda编程，C++并发编程4、补充关于操作系统，分布式系统的知识 近期打算","link":"/2025/05/17/Plan/"},{"title":"MAX30102 多通道 I2C 通讯与初始化问题分析","text":"本文记录在多通道 I2C 通讯过程中使用 MAX30102 传感器时遇到的问题及解决方案，适合用于多通道血氧模块或多传感器项目参考。 ==1. 通道选择问题：==通过 write(sensor, &amp;channel, 1) 来进行通道的选取和切换，但仅限于存在设备的通道可以正常扫描和初始化传感器。 问题原因：由于 write() 具有阻塞作用，当扫描到一个无设备的通道时会卡死，导致无法继续后续通道扫描。 解决思路：借助 Linux 系统自带的 i2ctools 工具进行通道扫描，使用如下命令： i2cset -y 4 0x70 0x01 i2cdetect -y 4 代码分享1234567891011121314151617181920212223242526272829303132void MAX30102::scanf_channel(){ for (int i = 0; i &lt; 8; i++) { char command[128]; snprintf(command, sizeof(command), &quot;i2cset -y 4 0x%02x 0x00 0x%02x&quot;, TCA9548A_ADDR, 1 &lt;&lt; i); if (system(command) != 0) { continue; } char detect_command[128]; snprintf(detect_command, sizeof(detect_command), &quot;i2cdetect -y 4 | grep -q '57'&quot;); if (system(detect_command) == 0) { enable_channels[count_channel++] = i; } else { std::cerr &lt;&lt; &quot;No device found at address 0x57 on channel &quot; &lt;&lt; std::endl; continue; } } for (int i = 0; i &lt; count_channel; i++) { std::cout &lt;&lt; &quot;channel : &quot; &lt;&lt; enable_channels[i] &lt;&lt; std::endl; }} ==2. 关于 I2C 总线设置问题：==2.1 通信频率与打开限制 I2C 总线最大频率为 400kHz； I2C 设备 不允许频繁重复打开； 否则会报错或导致设备失效(总线错误)。 2.2 通道切换与设备切换失败问题问题：切换了通道但设备没有跟随切换，原因是 write() 函数阻塞，导致 I2C 指向未切换成功的设备节点。 解决方法：先统一打开一次 I2C 总线，在实际访问设备时，动态返回配置后的设备句柄，并在使用后手动关闭以避免资源冲突。 示例代码：1234567891011121314151617int MAX30102::init_i2c(const char *device, int addr){ int temp_fd = open(device, O_RDWR); if (temp_fd == -1) { perror(&quot;Failed to open I2C device&quot;); return -1; } if (ioctl(temp_fd, I2C_SLAVE, addr) &lt; 0) { perror(&quot;Failed to set I2C address&quot;); close(temp_fd); return -1; } return temp_fd;}","link":"/2024/11/18/Linux%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91-max30102_tca9548/"},{"title":"机器学习一些进阶知识","text":"本文总结了机器学习中的贝叶斯决策、Fisher、SVM、核函数以及PCA降维等推理和操作。 贝叶斯决策最小错误贝叶斯决策贝叶斯公式我们希望根据观察到的特征向量 ( x )，判定其属于哪个类别 ( w_1 ) 或 ( w_2 )。使用贝叶斯定理计算后验概率： $$P(w_i|x) = \\frac{P(x|w_i) \\cdot P(w_i)}{P(x)} \\quad (i = 1,2)$$ 其中： ( P(w_i) )：类别的先验概率 ( P(x|w_i) )：在类别 ( w_i ) 下观察到 ( x ) 的概率（似然） ( P(x) )：边际概率，用于归一化 判别规则为了最小化分类错误率，我们选择使后验概率最大的类别： $$\\text{若 } P(w_1|x) &gt; P(w_2|x) \\Rightarrow x \\in w_1；\\quad \\text{否则 } x \\in w_2$$ 因为 ( P(x) ) 对所有类别相同，我们可以只比较分子部分： $$P(x|w_1) \\cdot P(w_1) \\quad \\text{vs} \\quad P(x|w_2) \\cdot P(w_2)$$ 判别函数形式定义判别函数： $$g_i(x) = P(x|w_i) \\cdot P(w_i), \\quad i = 1,2$$ 我们将样本 ( x ) 判为使 ( g_i(x) ) 最大的那一类。 最小风险贝叶斯决策在某些应用中，不同分类错误带来的代价不同。我们用损失函数 $$( \\lambda(\\alpha_i | w_j) )$$ 表示当真实类别为 $$( w_j )$$时，采取决策$$( \\alpha_i )$$的损失。 风险函数定义：$$R(\\alpha_i | x) = \\sum_{j=1}^{2} \\lambda(\\alpha_i | w_j) \\cdot P(w_j | x)$$ 决策准则：选择使期望风险最小的决策： $$\\alpha^* = \\arg\\min_{\\alpha_i} R(\\alpha_i | x)$$ 0-1 损失函数：退化为最大后验概率如果我们使用 0-1 损失： $$\\lambda(\\alpha_i|w_j) =\\begin{cases}0, &amp; i = j \\1, &amp; i \\ne j\\end{cases}$$ 则风险函数变为： $$R(\\alpha_i | x) = 1 - P(w_i|x)$$ 因此，最小风险决策等价于最大后验概率分类： $$\\alpha^* = \\arg\\max_i P(w_i | x)$$ 非对称损失示例在某些应用中，分类错误的代价不对称。设： 错将 ( w_2 ) 判为 ( w_1 ) 的损失为 10，即 $$( \\lambda(\\alpha_1 | w_2) = 10 )$$ 错将 $$( w_1 ) $$判为 $$( w_2 )$$ 的损失为 1，即$$ ( \\lambda(\\alpha_2 | w_1) = 1 )$$ 分类正确时损失为 0 因此，损失函数为： $$\\begin{cases}\\lambda(\\alpha_1 | w_1) = 0, &amp; \\lambda(\\alpha_1 | w_2) = 10 \\\\\\lambda(\\alpha_2 | w_1) = 1, &amp; \\lambda(\\alpha_2 | w_2) = 0\\end{cases}$$ 期望风险计算对每一个观测 ( x )，期望风险分别为： $$\\begin{aligned}R(\\alpha_1 | x) &amp;= \\lambda(\\alpha_1 | w_1) \\cdot P(w_1 | x) + \\lambda(\\alpha_1 | w_2) \\cdot P(w_2 | x) \\ &amp;= 0 \\cdot P(w_1 | x) + 10 \\cdot P(w_2 | x) = 10 \\cdot P(w_2 | x) \\\\R(\\alpha_2 | x) &amp;= \\lambda(\\alpha_2 | w_1) \\cdot P(w_1 | x) + \\lambda(\\alpha_2 | w_2) \\cdot P(w_2 | x) \\ &amp;= 1 \\cdot P(w_1 | x) + 0 \\cdot P(w_2 | x) = P(w_1 | x)\\end{aligned}$$ 决策准则选择期望风险较小的动作作为分类决策： $$\\text{如果 } 10 \\cdot P(w_2 | x) &lt; P(w_1 | x) \\Rightarrow x \\in w_1；\\quad \\text{否则 } x \\in w_2$$ 距离度量：欧式距离与马氏距离在分类、聚类、降维等机器学习任务中，衡量样本之间的距离非常关键。下面介绍两种常见的距离度量方法： 欧式距离（Euclidean Distance）欧式距离是最常见的距离度量，定义为两个点之间的直线距离。 若有两个 ( d ) 维向量 $$( \\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^d )$$，其欧式距离定义为： $$d_E(\\boldsymbol{x}, \\boldsymbol{y}) = \\sqrt{ \\sum_{i=1}^d (x_i - y_i)^2 } = | \\boldsymbol{x} - \\boldsymbol{y} |_2$$ 在二维空间下就是勾股定理，适合在特征方差接近、各维度独立同分布的场景。 马氏距离（Mahalanobis Distance）马氏距离考虑了各个维度之间的相关性以及不同尺度，是一种“归一化协方差后”的距离。 其定义为： $$d_M(\\boldsymbol{x}, \\boldsymbol{y}) = \\sqrt{ (\\boldsymbol{x} - \\boldsymbol{y})^T \\mathbf{S}^{-1} (\\boldsymbol{x} - \\boldsymbol{y}) }$$ 其中： $$( \\mathbf{S} )$$ 样本协方差矩阵 $$( \\mathbf{S}^{-1} )$$ 协方差矩阵的逆 $$( \\boldsymbol{x}, \\boldsymbol{y} )$$ 样本向量 当特征之间存在强相关性或不同尺度时，马氏距离能更准确地衡量“统计上的相似性”。 Fisher降维Fisher降维用途Fisher降维主要是通过投影的方法，使得数据映射到一条直线上面，维度从n维降低到1维，解决高维数据难以处理的问题。 基本数学推导: 支持向量机支持向量机(SVM)原理：支持向量机（SVM）是一种监督学习模型，通过在特征空间中寻找一个最优超平面，以最大化不同类别样本之间的间隔（margin），从而实现分类。它利用少量关键样本点（即支持向量）来决定分类边界，并通过核函数将数据映射到高维空间，处理线性不可分问题，实现高效、准确的分类。 公式推导过程: 核函数：用低维输入来描述高维关系在许多机器学习任务中，特别是支持向量机（SVM）与核主成分分析（KPCA）中，核函数（Kernel Function）是一种强大工具，能够用低维输入，隐式表达高维特征之间的非线性关系。 什么是核函数？核函数的本质是一种相似性度量函数。它通过一种特殊的函数 $$( K(\\boldsymbol{x}, \\boldsymbol{y}) )$$ 将两个输入向量映射到某个高维空间中，在不显式计算高维映射的前提下，完成复杂特征之间的内积计算。 形式定义为： $$K(\\boldsymbol{x}, \\boldsymbol{y}) = \\langle \\phi(\\boldsymbol{x}), \\phi(\\boldsymbol{y}) \\rangle$$ 其中： $$( \\phi(\\cdot) )$$ 从输入空间到高维特征空间的映射 $$( \\langle \\cdot, \\cdot \\rangle )$$ 在该空间中的内积 为什么需要核函数？非线性数据在原始特征空间中无法通过线性模型进行分割，如下图所示： 原空间中无法线性分离 经过高维映射后，线性可分 通过核函数，我们可以在计算上保持低维，在线性结构上获得高维的表达能力。 常见核函数下面是常见的几种核函数： 线性核： $$K(\\boldsymbol{x}, \\boldsymbol{y}) = \\boldsymbol{x}^T \\boldsymbol{y}$$ 不映射，直接在原空间中使用。 多项式核： $$K(\\boldsymbol{x}, \\boldsymbol{y}) = (\\boldsymbol{x}^T \\boldsymbol{y} + c)^d$$ 将数据映射到多项式特征空间。 高斯径向基核（RBF）： $$K(\\boldsymbol{x}, \\boldsymbol{y}) = \\exp\\left( -\\frac{|\\boldsymbol{x} - \\boldsymbol{y}|^2}{2\\sigma^2} \\right)$$ 实现到无限维空间的隐式映射，衡量局部相似性。 Sigmoid 核： $$K(\\boldsymbol{x}, \\boldsymbol{y}) = \\tanh(\\alpha \\boldsymbol{x}^T \\boldsymbol{y} + c)$$ 核技巧（Kernel Trick）核技巧是指：在训练算法中只依赖于输入样本的内积，我们可以用核函数替代原始内积，从而在无需显式进行高维映射的情况下，实现等效的线性建模。 例如，在 SVM 中的对偶形式中，模型依赖于所有支持向量的核函数： $$f(x) = \\sum_{i=1}^{n} \\alpha_i y_i K(\\boldsymbol{x}_i, \\boldsymbol{x}) + b$$ 这让非线性分类成为可能。 核函数如何使用核函数的核心思想是：在不显式进行高维映射的前提下，完成高维空间的计算。这称为“核技巧（Kernel Trick）”。 1. 为什么使用核函数？在某些问题中，数据在原始空间中是非线性不可分的，例如： 两类数据环绕分布（如圆环） 存在复杂曲线边界 我们希望将数据映射到一个更高维空间，在该空间中变得线性可分。但如果显式进行高维特征映射，会导致计算量急剧上升。 这时我们使用核函数 $$( K(x, x’) = \\langle \\phi(x), \\phi(x’) \\rangle )$$ 跳过显式映射，直接计算高维内积。 2. 在 SVM 中如何使用核函数？支持向量机的对偶形式使用的是样本之间的内积： $$f(x) = \\sum_{i=1}^n \\alpha_i y_i \\langle x_i, x \\rangle + b$$ 使用核函数后变成： $$f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b$$ 你只需要选好一个核函数，SVM 就能处理非线性问题！ 常见设置： 12345from sklearn.svm import SVC# 使用 RBF 核clf = SVC(kernel='rbf', gamma=0.5, C=1.0)clf.fit(X_train, y_train) PCA降维什么是 PCA？主成分分析（Principal Component Analysis, PCA） 是一种经典的无监督降维方法，它通过线性变换将原始数据投影到一个新的坐标系中，使得投影后的前几个维度保留数据中最多的信息（即方差最大）。 通俗讲：PCA 找到一组“最有代表性”的方向，将高维数据压缩到低维空间，同时尽量保留信息。 为什么需要降维？ 可视化：将高维数据投影到二维/三维便于观察 减少冗余：剔除无效或共线特征 降噪：提高泛化能力，减少过拟合 加速模型训练：特征更少，计算更快 PCA 的数学原理设原始数据集为 $$( X \\in \\mathbb{R}^{n \\times d} )，有 ( n ) 个样本、( d ) $$ 个特征。 步骤 1：中心化对每个特征去均值： $$X_{\\text{centered}} = X - \\mu$$ 步骤 2：计算协方差矩阵$$\\Sigma = \\frac{1}{n} X_{\\text{centered}}^T X_{\\text{centered}}$$ 步骤 3：特征值分解求解协方差矩阵的特征值和特征向量： $$\\Sigma \\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i$$ $$( \\boldsymbol{v}_i )$$ 第 ( i ) 个主成分方向 $$( \\lambda_i )$$ 对应的方差（解释力度） 步骤 4：选择前 ( k ) 个主成分取前 ( k ) 个最大特征值对应的特征向量组成投影矩阵 $$( W \\in \\mathbb{R}^{d \\times k} )$$ 步骤 5：变换到新空间$$Z = X_{\\text{centered}} \\cdot W$$ 其中 $$( Z \\in \\mathbb{R}^{n \\times k} )$$ 就是降维后的数据。 几何直觉PCA 等价于寻找一个新的坐标系，使得数据投影后的第一主轴方向具有最大方差，第二主轴方向次之，且各主轴正交。 可以类比于把数据“压扁”到一块高信息密度的平面上。 Python 示例：2D 可视化降维123456789101112131415from sklearn.datasets import load_irisfrom sklearn.decomposition import PCAimport matplotlib.pyplot as pltX, y = load_iris(return_X_y=True)# 降到二维pca = PCA(n_components=2)X_pca = pca.fit_transform(X)plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis')plt.xlabel(&quot;PC1&quot;)plt.ylabel(&quot;PC2&quot;)plt.title(&quot;Iris 数据集 PCA 降维&quot;)plt.show() 卷积核(cov)什么是卷积核？卷积核（Convolution Kernel）也称为滤波器（filter），是一个小型权重矩阵，它在输入数据（如图像）上滑动，执行局部加权求和操作，从而提取图像特征（如边缘、纹理、角点等）。 卷积操作的数学定义设有二维图像矩阵 ( I ) 和卷积核 ( K )，卷积操作记为： $$S(i, j) = (I * K)(i, j) = \\sum_m \\sum_n I(i+m, j+n) \\cdot K(m, n)$$ 其中： $$( I(i+m, j+n) )$$ 图像上某个局部区域的像素值 $$( K(m, n) )$$ 卷积核的对应位置权重 $$( S(i, j) )$$ 输出特征图的位置值 说明：实际计算中通常会将卷积核翻转（严格数学意义上的卷积），但在深度学习中使用的是互相关（cross-correlation），不翻转核。 举例：3×3 卷积核计算示意假设输入图像片段为： $$I =\\begin{bmatrix}1 &amp; 2 &amp; 0 \\4 &amp; 5 &amp; 1 \\1 &amp; 2 &amp; 3\\end{bmatrix}$$ 卷积核为： $$K =\\begin{bmatrix}0 &amp; 1 &amp; 0 \\1 &amp; -4 &amp; 1 \\0 &amp; 1 &amp; 0\\end{bmatrix}$$ 对应中心像素位置的卷积计算结果为： $$S = 1 \\cdot 0 + 2 \\cdot 1 + 0 \\cdot 0 + 4 \\cdot 1 + 5 \\cdot (-4) + 1 \\cdot 1 + 1 \\cdot 0 + 2 \\cdot 1 + 3 \\cdot 0 = -9$$ 卷积核在 CNN 中的作用在卷积神经网络（Convolutional Neural Network, CNN）中： 卷积核的参数是自动学习的； 每个卷积核可以学会提取一种特征（如边缘、曲线、角点）； 多个卷积核层叠使用，可以提取越来越抽象的语义信息。 一个典型的卷积层包含： 多个卷积核（如 32 个 3×3 核） 激活函数（如 ReLU） 池化层（如 MaxPooling） 可视化感受野与特征图当卷积核滑动时，它仅关注输入数据的一小块区域（感受野），并输出一个特征响应。多个卷积核叠加，可以形成特征图（Feature Map）： 输入图像 —→ 卷积核提取边缘 —→ 特征图（如轮廓）","link":"/2025/05/20/Maching_Learning(2)/"},{"title":"机器学习基础知识总结","text":"本文总结了机器学习中的核心概念、算法分类与应用案例。 Machine_Learning（1）1. 机器学习的分类机器学习主要分为 有监督学习 和 无监督学习 两大类。 有监督学习（Supervised Learning）定义：有监督学习是一种机器学习任务，其中每个训练样本都带有对应的标签（目标值）。模型通过学习这些“输入—输出”对，来预测新数据的输出。 典型任务： 分类（Classification）：预测离散标签，如垃圾邮件识别； 回归（Regression）：预测连续数值，如房价预测。 常见算法及说明： 线性回归（Linear Regression） 用途： 用于预测连续型数值，如房价、温度等。 原理： 建立输入特征与输出变量之间的线性关系模型 模型通过最小化均方误差（MSE）进行拟合。 逻辑回归（Logistic Regression） 用途： 用于二分类问题，输出事件发生的概率。 原理： 先计算线性组合（通过线性函数进行计算），再通过 Sigmoid 函数转换为概率。 支持向量机（SVM） 用途： 用于分类或回归，在高维空间中表现良好。 原理： SVM（支持向量机）是一种监督学习方法，其原理是通过在特征空间中构造一个最优超平面，将不同类别的样本最大间隔地分开，以提高模型的泛化能力；对于不可线性可分的情况，SVM通过核函数将数据映射到高维空间，使其在高维空间中线性可分，从而实现非线性分类。 K近邻（KNN） 用途： 用于分类与回归，基于样本相似度做预测。 原理： KNN（K-近邻算法）是一种基于距离度量的监督学习方法，其核心思想是：对一个待分类或预测的样本，给定已有的标记样本，找到训练集中距离它最近的K个样本，根据这些邻居的多数类别（投票）（分类）或平均数值（回归）来决定该样本的预测结果。 无监督学习（Unsupervised Learning）定义：无监督学习是一种机器学习任务，其中训练数据没有标签，模型需要自行发现数据中的模式、结构或分布。 典型任务： 聚类（Clustering）：将数据分成相似的组。 降维（Dimensionality Reduction）：压缩数据特征，减少冗余。 常见算法及说明： K-Means 聚类 用途： 用于将样本自动划分为预设的 K 个簇（类），常用于客户分群、图像分割等。 原理： 随机初始化 K 个聚类中心，然后迭代以下两个步骤直到收敛： 将每个样本分配给距离其最近的中心点； 更新每个簇的中心点为簇内所有样本的平均值。 特点： 简单高效，适用于大规模数据； 必须提前指定聚类数 K； 对初始中心敏感，可能陷入局部最优。 层次聚类（Hierarchical Clustering） 用途： 通过构建一个聚类的层次结构，实现自顶向下或自底向上的聚类，可视化强。 原理： 自底向上（凝聚型）：每个点开始作为一个单独簇，逐步合并最近的簇； 自顶向下（分裂型）：从一个整体出发，逐步分裂成更小的簇。 最终结果通常以树状图（Dendrogram）方式展示，可观察不同聚类层级。 特点： 不需要提前指定 K 值； 可生成多层次结构； 计算复杂度较高，不适合超大数据集。 主成分分析（PCA, Principal Component Analysis） 用途： 用于降维，压缩特征空间同时保留数据的主要信息，常用于可视化和特征提取。 原理： 通过线性变换找到一组新的正交坐标轴（主成分）； 第一个主成分具有最大方差，第二个主成分与第一个正交，且具有次大方差，依此类推； 最终保留前 K 个主成分，用于表示数据。 特点： 降维效果好，计算效率高； 可去除特征冗余、提高模型泛化能力； 属于线性方法，难以处理非线性特征关系。 2. 应用实例对比 类别 应用示例 输入数据类型 输出结果 有监督学习 邮件分类 邮件内容（文本） 是否为垃圾邮件 有监督学习 房价预测 房屋面积、地段等 房价（连续值） 无监督学习 客户分群 用户购买记录、访问频率 用户分类 无监督学习 图像降维 像素矩阵 降维后向量表示 3. 总结 有监督学习 适合解决“有标签”的任务，如分类与回归； 无监督学习 更注重发现数据内部结构； 掌握它们的区别，有助于我们在实际场景中选择合适的算法。","link":"/2025/05/18/Machine_Learning(1)/"},{"title":"tensorRT基础操作","text":"本文介绍TensorRT10.3.0的编程基础。 1、创建引擎1、创建log日志123456789101112class Logger : public ILogger{public: void log(Severity severity, const char *msg) noexcept override { if (severity &lt;= Severity::kWARNING) { std::cout &lt;&lt; msg &lt;&lt; std::endl; } }}; 2、创建推理构建器1IBuilder *builder = createInferBuilder(logger); 3、构建网络12uint32_t flag = 0; // 使用默认标志，不使用 kEXPLICIT_BATCHINetworkDefinition *network = builder-&gt;createNetworkV2(flag); 4、加载onnx模型并解析123456789101112131415161718192021222324252627// 创建 ONNX 解析器 IParser *parser = createParser(*network, logger); // 解析 ONNX 模型文件 const std::string onnxModelPath = &quot;model.onnx&quot;; std::ifstream modelFile(onnxModelPath, std::ios::binary); if (!modelFile) { std::cerr &lt;&lt; &quot;无法打开模型文件：&quot; &lt;&lt; onnxModelPath &lt;&lt; std::endl; return -1; } modelFile.seekg(0, std::ios::end); size_t modelSize = modelFile.tellg(); modelFile.seekg(0, std::ios::beg); std::vector&lt;char&gt; modelData(modelSize); modelFile.read(modelData.data(), modelSize); modelFile.close(); if (!parser-&gt;parse(modelData.data(), modelSize)) { std::cerr &lt;&lt; &quot;解析 ONNX 模型失败：&quot; &lt;&lt; std::endl; for (int i = 0; i &lt; parser-&gt;getNbErrors(); ++i) { std::cerr &lt;&lt; parser-&gt;getError(i)-&gt;desc() &lt;&lt; std::endl; } return -1; } 5、构建配置并且序列化1234567891011121314151617181920212223242526// 创建构建配置 IBuilderConfig *config = builder-&gt;createBuilderConfig(); // config-&gt;setMaxWorkspaceSize(1U &lt;&lt; 30); // 设置最大工作空间大小为 1GB // 序列化网络并构建引擎 IHostMemory *serializedModel = builder-&gt;buildSerializedNetwork(*network, *config); if (!serializedModel) { std::cerr &lt;&lt; &quot;构建序列化网络失败&quot; &lt;&lt; std::endl; return -1; } // 将序列化的引擎保存到文件 const std::string engineFilePath = &quot;model.engine&quot;; std::ofstream engineFile(engineFilePath, std::ios::binary); engineFile.write(reinterpret_cast&lt;const char *&gt;(serializedModel-&gt;data()), serializedModel-&gt;size()); engineFile.close(); // 清理资源 delete parser; delete network; delete config; delete builder; std::cout &lt;&lt; &quot;引擎构建并保存成功：&quot; &lt;&lt; engineFilePath &lt;&lt; std::endl; return 0; 6、完整基础代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#include &lt;NvInfer.h&gt;#include &lt;NvOnnxParser.h&gt;#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;vector&gt;using namespace nvinfer1;using namespace nvonnxparser;class Logger : public ILogger{public: void log(Severity severity, const char *msg) noexcept override { if (severity &lt;= Severity::kWARNING) { std::cout &lt;&lt; msg &lt;&lt; std::endl; } }};int main(){ // 创建日志记录器 Logger logger; // 创建推理构建器 IBuilder *builder = createInferBuilder(logger); // 创建网络定义，使用 kDEFAULT 来替代 kEXPLICIT_BATCH uint32_t flag = 0; // 使用默认标志，不使用 kEXPLICIT_BATCH INetworkDefinition *network = builder-&gt;createNetworkV2(flag); // 创建 ONNX 解析器 IParser *parser = createParser(*network, logger); // 解析 ONNX 模型文件 const std::string onnxModelPath = &quot;model.onnx&quot;; std::ifstream modelFile(onnxModelPath, std::ios::binary); if (!modelFile) { std::cerr &lt;&lt; &quot;无法打开模型文件：&quot; &lt;&lt; onnxModelPath &lt;&lt; std::endl; return -1; } modelFile.seekg(0, std::ios::end); size_t modelSize = modelFile.tellg(); modelFile.seekg(0, std::ios::beg); std::vector&lt;char&gt; modelData(modelSize); modelFile.read(modelData.data(), modelSize); modelFile.close(); if (!parser-&gt;parse(modelData.data(), modelSize)) { std::cerr &lt;&lt; &quot;解析 ONNX 模型失败：&quot; &lt;&lt; std::endl; for (int i = 0; i &lt; parser-&gt;getNbErrors(); ++i) { std::cerr &lt;&lt; parser-&gt;getError(i)-&gt;desc() &lt;&lt; std::endl; } return -1; } // 创建构建配置 IBuilderConfig *config = builder-&gt;createBuilderConfig(); // config-&gt;setMaxWorkspaceSize(1U &lt;&lt; 30); // 设置最大工作空间大小为 1GB // 序列化网络并构建引擎 IHostMemory *serializedModel = builder-&gt;buildSerializedNetwork(*network, *config); if (!serializedModel) { std::cerr &lt;&lt; &quot;构建序列化网络失败&quot; &lt;&lt; std::endl; return -1; } // 将序列化的引擎保存到文件 const std::string engineFilePath = &quot;model.engine&quot;; std::ofstream engineFile(engineFilePath, std::ios::binary); engineFile.write(reinterpret_cast&lt;const char *&gt;(serializedModel-&gt;data()), serializedModel-&gt;size()); engineFile.close(); // 清理资源 delete parser; delete network; delete config; delete builder; std::cout &lt;&lt; &quot;引擎构建并保存成功：&quot; &lt;&lt; engineFilePath &lt;&lt; std::endl; return 0;} 当然也可以如下自定义网络不用onnx：12345678910111213141516171819202122INetworkDefinition* network = builder-&gt;createNetworkV2(0);// 创建输入层auto input = network-&gt;addInput(&quot;input&quot;, DataType::kFLOAT, Dims3(1, 28, 28));// 创建卷积层auto conv = network-&gt;addConvolution(*input, 32, DimsHW(3, 3), weightTensor, biasTensor);conv-&gt;setStride(DimsHW(1, 1));// 创建 ReLU 激活层auto relu = network-&gt;addActivation(*conv-&gt;getOutput(0), ActivationType::kRELU);// 创建池化层auto pool = network-&gt;addPooling(*relu-&gt;getOutput(0), PoolingType::kMAX, DimsHW(2, 2));pool-&gt;setStride(DimsHW(2, 2));// 创建输出层auto output = network-&gt;addOutput(*pool-&gt;getOutput(0));// 创建引擎配置并构建引擎IBuilderConfig* config = builder-&gt;createBuilderConfig();ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); 7、序列化的意义TensorRT 序列化的作用和意义主要体现在以下几个方面： 1. 提高推理效率TensorRT 序列化的主要作用之一是将经过优化的网络模型保存为一个二进制文件（通常是 .engine 文件）。这个文件包含了网络的结构、权重以及与推理相关的优化信息。在推理时，TensorRT 可以直接加载这个序列化的引擎文件，而不需要每次都重新解析原始的 ONNX 模型或进行优化。这样可以显著减少推理的初始化时间，并提高推理的效率。 2. 优化推理性能通过序列化，TensorRT 会在创建引擎的过程中进行一系列的优化，例如： 层融合：将多个算子融合成一个更高效的算子，减少计算量。 精度降低：通过使用 FP16 或 INT8 等低精度进行推理，以提高速度并减少内存使用。 内存管理：优化内存的使用方式，使得内存的占用最小化，尤其是在 GPU 上，能够有效利用显存。 动态批量大小支持：序列化后的引擎通常支持动态批量大小，这意味着引擎可以根据实际输入的批量大小自动调整，从而避免了不必要的内存浪费。 3. 便于模型部署通过序列化，TensorRT 将网络模型变成了一个标准化的文件（.engine），这个文件可以被部署到不同的环境中。例如： 在 GPU 上进行推理时，.engine 文件可以直接加载并执行，而无需重新构建或重新解析原始模型。 该序列化引擎也可以在不同的平台之间共享，如从开发机器迁移到生产环境，或者在不同的硬件设备上进行推理。 序列化使得模型从训练到部署的过程变得更加高效和简单。 4. 节省存储和计算资源 减少加载时间：与直接从原始 ONNX 模型文件加载相比，加载已序列化的 TensorRT 引擎文件速度要快得多。序列化的引擎已经包含了所有必要的优化和硬件加速信息，所以加载时间大大缩短。 减少计算资源消耗：由于 TensorRT 在序列化过程中已应用了多种优化，重新加载并执行时，所需的计算资源（如 GPU 核心数、内存等）更少，推理速度更快。 5. 与硬件加速紧密结合TensorRT 序列化后的引擎文件通常会根据目标硬件（如 GPU 类型）进行特定优化。例如： 如果引擎在 NVIDIA A100 GPU 上序列化，它可能会利用 A100 GPU 上的特定硬件加速功能（如 Tensor Cores）。 序列化后的引擎文件可以直接使用硬件特性进行推理，而无需每次都进行繁重的优化步骤。 6. 支持多种精度TensorRT 序列化时，可以指定精度模式（如 FP32、FP16 或 INT8）。通过在序列化过程中将精度降低为较低精度（如 FP16 或 INT8），可以大幅提高推理速度，并减少显存占用。这些低精度模式对于推理任务非常重要，尤其是在性能和功耗要求严格的环境中（如边缘设备）。 7. 简化部署和集成使用 TensorRT 序列化的引擎文件，用户不需要重新编译或重新构建推理代码。只需要将引擎文件和相应的加载代码部署到目标设备上，程序就可以直接加载并进行推理。这种方式简化了部署过程，并减少了配置和测试的复杂性。 2、使用引擎推理1、加载引擎并且反序列化1234567891011121314151617181920212223242526272829303132ICudaEngine *loadEngine(const std::string &amp;engineFilePath, Logger &amp;logger){ std::ifstream engineFile(engineFilePath, std::ios::binary); if (!engineFile) { std::cerr &lt;&lt; &quot;无法打开引擎文件：&quot; &lt;&lt; engineFilePath &lt;&lt; std::endl; return nullptr; } // 读取引擎文件 std::vector&lt;char&gt; engineData((std::istreambuf_iterator&lt;char&gt;(engineFile)), std::istreambuf_iterator&lt;char&gt;()); engineFile.close(); // 创建 TensorRT 运行时 IRuntime *runtime = createInferRuntime(logger); if (!runtime) { std::cerr &lt;&lt; &quot;无法创建推理运行时&quot; &lt;&lt; std::endl; return nullptr; } // 反序列化引擎 ICudaEngine *engine = runtime-&gt;deserializeCudaEngine(engineData.data(), engineData.size()); if (!engine) { std::cerr &lt;&lt; &quot;无法反序列化引擎&quot; &lt;&lt; std::endl; return nullptr; } return engine;} 在使用 TensorRT 序列化引擎时，反序列化的过程是将保存为 .engine 文件的序列化引擎加载到内存中，并准备好用于推理。反序列化过程的主要目的是将之前序列化并保存的引擎文件转换回一个可以执行推理的对象。 2、创建上下文进行推理12345678910111213141516171819202122void doInference(ICudaEngine *engine, void *inputData, void *outputData){ // 创建执行上下文 IExecutionContext *context = engine-&gt;createExecutionContext(); if (!context) { std::cerr &lt;&lt; &quot;无法创建执行上下文&quot; &lt;&lt; std::endl; return; } // 分配输入输出缓冲区 void *buffers[2]; buffers[0] = inputData; // 输入数据 buffers[1] = outputData; // 输出数据 // 执行推理 context-&gt;executeV2(buffers); // 销毁执行上下文 delete context;} 3、使用实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354int main(){ // 创建日志记录器 Logger logger; // 加载已序列化的引擎 const std::string engineFilePath = &quot;model.engine&quot;; ICudaEngine *engine = loadEngine(engineFilePath, logger); if (!engine) { std::cerr &lt;&lt; &quot;加载引擎失败&quot; &lt;&lt; std::endl; return -1; } // 准备输入输出数据 int inputSize = 1 * 3 * 224 * 224; // 假设输入是一个 RGB 图像，大小为 224x224 int outputSize = 1 * 1000; // 假设输出是 1000 维的概率分布 float *inputData = new float[inputSize]; // 你可以在这里加载输入数据 float *outputData = new float[outputSize]; // 用于保存输出数据 // 在 GPU 上分配内存 void *inputDevice = nullptr; void *outputDevice = nullptr; cudaMalloc(&amp;inputDevice, inputSize * sizeof(float)); // 分配输入内存 cudaMalloc(&amp;outputDevice, outputSize * sizeof(float)); // 分配输出内存 // 将输入数据从主机拷贝到 GPU cudaMemcpy(inputDevice, inputData, inputSize * sizeof(float), cudaMemcpyHostToDevice); // 执行推理 doInference(engine, inputDevice, outputDevice); // 获取输出数据，从 GPU 拷贝回主机 cudaMemcpy(outputData, outputDevice, outputSize * sizeof(float), cudaMemcpyDeviceToHost); // 输出前五个结果 for (int i = 0; i &lt; 5; i++) { std::cout &lt;&lt; &quot;Output[&quot; &lt;&lt; i &lt;&lt; &quot;] = &quot; &lt;&lt; outputData[i] &lt;&lt; std::endl; } // 清理资源 delete[] inputData; delete[] outputData; cudaFree(inputDevice); cudaFree(outputDevice); // 销毁引擎 delete engine; return 0;} 3、关于tensorRT的优化TensorRT 的高性能推理优化方法主要体现在以下几个方面： **层融合 (Layer Fusion)**： TensorRT 会将多个神经网络中的层进行融合，将这些操作合并为一个更高效的计算内核，从而减少内存访问和计算开销。例如，将卷积层和激活函数层合并为一个操作，以减少中间结果的存储需求和不必要的数据传输。 这种层融合能够显著提升推理效率，因为每次内存访问和计算都需要时间，融合多个操作后可以减少这些操作之间的中间数据传输。 **内核自动调优 (Kernel Auto-Tuning)**： 针对不同的 GPU 架构，TensorRT 会自动选择最优的计算内核。这意味着 TensorRT 可以为每种不同的硬件（例如 NVIDIA 的 Volta 或 Turing 架构 GPU）选择最适合的计算内核实现，从而最大化性能。 自动调优的核心思想是通过试验不同的内核实现和参数设置，选出最合适的内核配置，以达到最佳的推理速度。 **动态张量内存管理 (Dynamic Tensor Memory Management)**： TensorRT 对内存使用进行优化，自动管理张量内存，减少内存占用并降低数据传输时间。通过精确控制内存的分配与回收，TensorRT 能够避免不必要的内存开销，从而提升推理效率。 在推理过程中，TensorRT 会根据需要动态调整内存大小，减少内存碎片，并通过优化的数据传输方式提高内存带宽的利用率，最终减少推理延迟。 这些优化使得 TensorRT 在推理任务中能够显著提高速度，尤其在大规模模型部署时，能够充分发挥硬件的性能优势。","link":"/2025/05/19/TensorRT%E5%9F%BA%E7%A1%80/"},{"title":"YOLO原理","text":"本文主要介绍了YOLO目标检测的结构以及原理 YOLO目标检测YOLO目标检测的目的：给定一张图片或者一帧视频，设计好模型定位图片中我们感兴趣的物体，同时标注其所属类别 目标检测的原理和算法： 目标检测基本框架：Backbone network: 主干网络，实现从图像检测中提取出必要的特征信息，然后利用特征去实现定位和分类。 常见主干网络模型 网络名 特点 用途示例 VGG 堆叠简单卷积层，结构规则 早期检测器（如 SSD） ResNet 引入残差连接，解决深层退化问题 Faster R-CNN, Mask R-CNN MobileNet 深度可分离卷积，轻量高效 适用于移动端模型部署 EfficientNet 复合缩放策略，精度与效率均衡 轻量检测器如 YOLOv7-tiny DenseNet 层间全连接，特征复用强 特征提取、语义分割任务 CSPNet 改进的梯度流与参数分离策略 YOLOv4/v5/YOLOv8 主干 Swin Transformer 视觉Transformer结构，支持长程建模 最新版本检测器（如 DINO） 关于ResNet引言在深度学习中，随着网络层数的加深，模型表现并不总是提升，反而可能出现退化问题——更深的网络训练误差反而更高。为了解决这一问题，微软研究院于 2015 年提出了 ResNet（Residual Network），成为深度网络设计的里程碑。 ResNet 的核心思想：残差连接传统卷积网络学习的是： $$y = \\mathcal{F}(x)$$ ResNet 则改为学习： $$y = \\mathcal{F}(x) + x$$ 其中： ( x )：输入特征 $$( \\mathcal{F}(x) )$$ 残差映射（通常是若干层卷积） $$( x + \\mathcal{F}(x) )$$ 称为残差连接或跳跃连接（skip connection） 这种结构允许信息直接“跳过”卷积层，缓解梯度消失、退化等问题。 ResNet的本质就是，加上x，做恒等映射，把之前网络的梯度传递下来，使得梯度可以保留，同时x可以为0，即没有学到任何信息最优输出为自己本身，不会对模型造成影响。 加上残差连接（+x）后，网络在前向传播中保留了原始输入特征，在反向传播中确保了梯度能直接传回前层，避免梯度消失，从而使网络更容易训练、可以加深，并能高效学习“偏离输入”的差异，而不是从零开始学习全部映射。 ResNet 的基本模块：残差块（Residual Block）标准残差块结构如下图所示：123456Input │ ├───► Conv → BN → ReLU → Conv → BN ───┐ │ │ └──────────────────► Elementwise Add ──► ReLU Neck Network（颈部网络）位于检测模型的 Backbone（主干网络）与 Head（检测头）之间，是连接提取与预测的桥梁。 其主要作用是： 多尺度特征融合：整合来自不同层次的语义和空间信息； 增强上下文感知能力：提升模型对大物体、小物体的识别能力； 信息传导通路：为后续分类与定位提供统一格式的特征张量。 为什么需要 Neck？主干网络输出的特征通常存在分辨率差异： 高层特征：语义强但空间分辨率低（适合识别整体类别）； 低层特征：空间精度高但语义弱（适合定位边缘或小目标）； Neck 的任务就是将这些不同层级的特征进行融合与传递，使得检测头可以“既懂语义，又知细节”。 常见 Neck 结构对比 结构 全称 特点说明 FPN Feature Pyramid Network 自顶向下特征融合，提升小目标检测能力 PANet Path Aggregation Network 引入自底向上路径，增强位置敏感特征 BiFPN Bidirectional Feature Pyramid 双向融合 + 加权特征选择，用于高效检测 YOLO-Nano Neck C2f + SPPF + upsample/concat YOLOv8 使用轻量结构实现高效上下文建模 关于FPN ： FPN就是在检测之前，先将多个尺度的特征图进行一次buttom-up的融合 FPN 的工作流程包括三个步骤： Bottom-up（从低到高）路径： 由主干网络（如 ResNet）生成多层级特征图 $$( C_2, C_3, C_4, C_5 )$$ 特征越往上越抽象、越语义化。 Top-down（从高到低）路径： 从顶层特征开始，逐层进行上采样(放大图像)（如 nearest 或 bilinear）； 每一级都与相应的 bottom-up 特征图横向连接（1×1卷积后相加）； 实现语义信息的向下传递。 横向连接（Lateral connections）： 在每一层使用卷积对底层特征进行通道变换； 与上采样后的高层语义特征相加，完成融合。 融合后的特征图记为 ( P_2, P_3, P_4, P_5 )，每个尺度都具备了丰富语义 + 精细结构，非常适合目标检测中的多尺度预测任务。 应用场景 小目标检测（提升显著） 多尺度目标检测（如 RetinaNet、Mask R-CNN） YOLO 系列中也借鉴 FPN 原理构建 Neck（如 PANet） 关于SPP（Spatial Pyramid Pooling）空间金字塔池化SPP（空间金字塔池化） 是一种用于增强神经网络感受野的结构，最早应用于图像分类任务中，后来在 YOLOv3~YOLOv5 等目标检测模型中广泛使用。 它的目标是：在不改变输入尺寸的前提下，提取多尺度空间信息并融合，从而提升模型对不同大小目标的识别能力。 为什么需要 SPP？传统的卷积 + 池化结构只能捕捉固定尺度的局部信息，但目标可能有不同的形状和大小，SPP 提供了一种多尺度上下文建模方式： 小池化核：捕捉局部精细结构； 大池化核：感知全局上下文区域。 通过多尺度池化输出连接，模型能同时理解图像的局部细节和整体语义。 SPP 工作原理SPP 模块会在一张特征图上，并行使用多个不同大小的池化核（如 5×5、9×9、13×13），然后将它们的输出与原始特征图进行 拼接（concat）。 1234567891011 输入特征图 │ ┌─────────┼────────────┐ │ │ │5×5池化 9×9池化 13×13池化 │ │ │ └────┬────┴─────┬──────┘ ↓ ↓ 通道拼接（concat） ↓ 输出特征图 池化（Pooling）操作详解在卷积神经网络（CNN）中，池化（Pooling） 是一种用于降低特征图空间维度的操作，能够压缩信息、减少计算量，并提升模型的平移不变性和鲁棒性。 1. 池化的作用 降低特征图尺寸（减少内存占用与计算负担） 提取主干特征（保留最显著的响应） 增强局部不变性（使模型对位置偏移更鲁棒） 防止过拟合（通过信息压缩减少冗余） 2. 常见池化类型 类型 说明 最大池化（Max Pooling） 每个滑动窗口中取最大值 平均池化（Average Pooling） 每个窗口取平均值 全局平均池化（Global Average Pooling） 整个特征图平均，变为一个数 3. 池化操作示意以 2×2 最大池化为例： 123456789101112输入特征图（4×4）：[[1, 3, 2, 1], [4, 6, 5, 2], [1, 0, 2, 4], [3, 2, 1, 5]]→ 使用 2×2 滑动窗口 + 步长 2输出特征图（2×2）：[[6, 5], [3, 5]] Detection Head：负责定位与检测Detection Head（检测头） 是目标检测模型中最终完成预测的模块，接收来自 Backbone + Neck 网络融合后的多尺度特征图，输出每个位置上的分类结果和回归坐标。 检测头的作用检测头对特征图进行卷积，目的是从每个位置提取该位置是否存在目标、目标属于哪类、目标框的位置这些局部感知信息。 分类分支：判断该位置上属于哪个类别（或是否为前景）； 回归分支：预测边界框的位置参数（通常是中心坐标 + 宽高，或偏移量形式）； 通常每个尺度特征图都单独接一个检测头，多尺度检测能力来源于此。 结构特点检测头通常非常简单，主要由几层小卷积 + 输出层构成： 不需要复杂结构； 不进行特征提取，仅负责最终特征转输出； 通常是轻量的，参数占比不大，但作用关键。 检测头本质上就是在每一个特征图位置上卷积滑窗计算分类和回归输出。它结构简单，执行高效，但承担着模型预测的全部核心逻辑。 RetinaNet 中的检测头示意 在 RetinaNet 中： 每个 FPN 层输出的特征图都接两个独立分支： 一个分类头（Class Head）：多个 3×3 卷积 + sigmoid 输出 一个回归头（Box Head）：多个 3×3 卷积 + 边界框输出 两者结构相似，但参数不共享，各自独立优化。 YOLOV8训练时候的参数指标： 指标名 含义 常见范围 说明 box_loss 边界框回归损失 越低越好 表示预测框与真实框之间的差异（如 IoU Loss） cls_loss 分类损失 越低越好 表示分类误差，通常采用交叉熵 dfl_loss 分布式回归损失 边界框回归，预测框的位置分布与真实样本坐标分布接近,越低越好 用于增强回归精度 loss 总损失 越低越好 综合损失，越小模型表现越好 mAP50 平均精度（IoU=0.5） [0, 1] 用于粗略衡量模型检测性能 mAP50-95 平均精度（IoU=0.5~0.95） [0, 1] 更全面、更严格的性能评估指标 在目标检测中，预测框和真实框之间的重叠面积占总体面积的比例就是 IoU 🔗 总损失公式YOLO（以 Ultralytics YOLOv8 为例）的总损失由三项加权求和： $$(\\mathcal{L}_{\\mathrm{box}})$$ 边框回归损失（CIoU/DIoU/GIoU），对应 box_loss $$(\\mathcal{L}_{\\mathrm{cls}})$$ 分类损失（Focal Loss 或 BCEWithLogitsLoss），对应 cls_loss $$(\\mathcal{L}_{\\mathrm{dfl}})$$ Distribution Focal Loss，对应 dfl_loss $$(\\lambda_{\\mathrm{box}}, \\lambda_{\\mathrm{cls}}, \\lambda_{\\mathrm{dfl}}) $$是在 hyp.yaml 中定义的权重，用于平衡三者。 ⚙️ 默认超参数权重Ultralytics YOLOv8 默认的损失权重设置如下： 1234hyp: box: 0.05 # 边框回归损失权重 cls: 0.50 # 分类损失权重 dfl: 1.50 # Distribution Focal Loss 权重 参数介绍1. Loss 趋势 初期 loss 应迅速下降； 中后期趋于平稳或缓慢下降； 若 loss 不降反升，可能出现梯度爆炸或过拟合。 2. mAP 表现 mAP50-95 &gt; 0.5 表示模型基本合格； mAP &gt; 0.7 可视为优秀模型； mAP50 较高但 mAP50-95 较低可能说明检测框偏差大。 3. 收敛与过拟合 验证 mAP 多轮无提升或下降：考虑早停或调参； 验证集 mAP 下降但训练集 loss 持续下降：过拟合信号。 YOLO参数解读 📉 训练与验证损失（Loss）分析损失函数衡量的是模型预测与实际标注之间的差距，下降趋势表明模型正在逐步学习正确的特征。 train/box_loss 与 val/box_loss 定义：边框回归损失，用于衡量预测框与真实框之间的位置差异。 变化趋势：逐渐下降，表明模型定位能力在逐步增强。 train/cls_loss 与 val/cls_loss 定义：分类损失，用于衡量预测类别与真实类别之间的差异。 变化趋势：前期快速下降，后期趋于平稳，说明模型快速掌握基本分类规则，并进行精细调整。 train/dfl_loss 与 val/dfl_loss 定义：分布式焦点损失（Distribution Focal Loss），优化边框回归精度。 变化趋势：整体稳步下降，表明预测框的定位精度持续提升。 📊 模型性能评估指标（Metrics）在目标检测任务中，评估指标直接体现模型在真实应用场景中的表现。 metrics/precision(B) 定义：精确率，表示预测为正的样本中实际为正的比例。 趋势：持续上升，误检率降低，模型预测更精准。 metrics/recall(B) 定义：召回率，表示所有真实正样本中被正确识别的比例。 趋势：逐步上升，说明模型对正样本的覆盖能力增强。 metrics/mAP50(B) 定义：在 IoU 阈值为 0.5 时的平均精度均值（Mean Average Precision）。 趋势：由初期的 0.2 上升至 0.6，说明模型准确度显著提升。 metrics/mAP50-95(B) 定义：多个 IoU 阈值（0.5 至 0.95）下的平均精度，更严格的衡量标准。 趋势：逐步提升，从 0.15 增至 0.37，显示模型在严格要求下也具有良好表现。","link":"/2025/05/25/YOLO%E5%9F%BA%E7%A1%80/"},{"title":"关于 thread 线程和 POSIX 的应用","text":"本文介绍了在项目开发过程中的thread问题以及高精度定时器POSIX的使用出现的问题 1. 高精度计时器 POSIX 的使用一般的应用：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &quot;MaxDataWorker.h&quot;#include &quot;max30102.h&quot;#include &lt;iostream&gt;#include &lt;chrono&gt;#include &lt;thread&gt;#include &lt;signal.h&gt;using namespace std;using namespace chrono;int datacount = 0;void timer_handler(int sig, siginfo_t *si, void *uc){ MAX30102 *data = (MAX30102 *)si-&gt;si_value.sival_ptr; data-&gt;get_data(); datacount++;}MaxDataWorker::MaxDataWorker(QObject *parent, MAX30102 *sensor) : QObject(parent), max30102(sensor){}MaxDataWorker::~MaxDataWorker(){}void MaxDataWorker::doWork(){ struct sigevent sev; struct sigaction sa; sa.sa_flags = SA_SIGINFO; sa.sa_sigaction = timer_handler; sigemptyset(&amp;sa.sa_mask); sigaction(SIGRTMIN, &amp;sa, NULL); sev.sigev_notify = SIGEV_SIGNAL; sev.sigev_signo = SIGRTMIN; sev.sigev_value.sival_ptr = max30102; timer_t timerid; if (timer_create(CLOCK_REALTIME, &amp;sev, &amp;timerid) == -1) { perror(&quot;timer_create&quot;); } struct itimerspec its; its.it_value.tv_sec = 0; its.it_value.tv_nsec = 12500000; its.it_interval.tv_sec = 0; its.it_interval.tv_nsec = 12500000; if (timer_settime(timerid, 0, &amp;its, NULL) == -1) { perror(&quot;timer_settime&quot;); } std::this_thread::sleep_for(std::chrono::seconds(5)); timer_delete(timerid); cout &lt;&lt; &quot;datacount is :&quot; &lt;&lt; datacount &lt;&lt; endl; emit finishRead();} 需要注意的是： sa.sa_sigaction = timer_handler;：只能使用普通函数或静态成员函数。 如果需要访问类成员函数，需通过 sigev_value.sival_ptr 传递对象指针： 123456void timer_handler(int sig, siginfo_t *si, void *uc){ MAX30102 *data = (MAX30102 *)si-&gt;si_value.sival_ptr; data-&gt;get_data(); datacount++;} 1sev.sigev_value.sival_ptr = max30102; POSIX 定时器本身就是一个线程。 2. 关于线程 QThread 和 std::thread可参考：C++ 与 Qt 线程：C++ std::thread 与 Qt QThread 多线程混合编程 3. QThread 与 POSIX 的使用 QThread 与 POSIX 均可用信号机制，不会冲突。 使用 pthread 启动 POSIX 定时器时，可能引发信号干扰，影响其他 QThread 的信号处理。 解决方案：在 MaxPlot.cpp 全局使用 POSIX 定时器，它本身就是线程，可定时触发数据读取逻辑。 4. 多线程竞争与原子操作问题当定时器频繁触发同一方法，若前一次调用未完成，可能导致竞态问题。 可使用 std::atomic 原子操作避免冲突： 123456789101112131415161718192021222324252627282930void MaxPlot::posix_timer_handler(union sigval sv){ MAX30102 *data = static_cast&lt;MAX30102 *&gt;(sv.sival_ptr); bool expected = false; if (data-&gt;is_reading.compare_exchange_strong(expected, true)) { data-&gt;get_data(); data-&gt;datacount++; if (data-&gt;datacount &gt;= 360) { std::cout &lt;&lt; &quot;data is up to full&quot; &lt;&lt; std::endl; data-&gt;datacount = 0; emit data-&gt;finishRead(); data-&gt;Quit(); cout &lt;&lt; &quot;Jump is:&quot; &lt;&lt; data-&gt;count_Jump &lt;&lt; endl; data-&gt;count_Jump = 0; } data-&gt;is_reading = false; } else { std::cerr &lt;&lt; &quot;上一次读取未完成，跳过此次读取&quot; &lt;&lt; std::endl; data-&gt;count_Jump++; data-&gt;datacount++; }}","link":"/2025/05/17/%E5%85%B3%E4%BA%8Ethread%E7%BA%BF%E7%A8%8B%E5%92%8CPOSIX%E7%9A%84%E5%BA%94%E7%94%A8/"},{"title":"MQTT 与 HTTP 通信协议对比及项目应用场景选择","text":"本文总结了 MQTT 与 HTTP 协议的区别、各自的适用场景，并结合项目案例说明了前后端通信选型理由。 ==1. MQTT 通信与网络协议==相关学习资料： MQTT协议详解(完整版) - CSDN博客 MQTT协议史上最全解析 - CSDN博客 TCP/IP协议图解 - CSDN博客 HTTP协议详解 - CSDN博客 MQTT 示例代码（C）：1234567891011121314151617181920MQTTClient client;MQTTClient_connectOptions conn_opts = MQTTClient_connectOptions_initializer;MQTTClient_create(&amp;client, ADDRESS, CLIENTID, MQTTCLIENT_PERSISTENCE_NONE, NULL);int rc;if ((rc = MQTTClient_connect(client, &amp;conn_opts)) != MQTTCLIENT_SUCCESS){ printf(&quot;Failed to connect, return code %d\\n&quot;, rc); exit(EXIT_FAILURE);}char payload[100];sprintf(payload, &quot;Middle Data : %u, %u&quot;, red_data, ir_data);MQTTClient_message pubmsg = MQTTClient_message_initializer;pubmsg.payload = payload;pubmsg.payloadlen = strlen(payload);pubmsg.qos = QOS;pubmsg.retained = 0;MQTTClient_deliveryToken token;MQTTClient_publishMessage(client, TOPIC, &amp;pubmsg, &amp;token);MQTTClient_waitForCompletion(client, token, TIMEOUT);printf(&quot;Message with delivery token %d delivered\\n&quot;, token); 2.前后端交互(API)的使用 什么是API： API（应用程序编程接口，Application Programming Interface）是一组定义不同软件组件之间如何相互通信和交互的规范和协议。API 充当了不同软件系统之间的桥梁，使它们能够无缝协作，而无需了解彼此的内部实现细节。 如何理解API，API 是如何工作的-CSDN博客 3.HTTP和MQTT的选择要区分什么时候使用MQTT，什么时候使用HTTP，我们需要弄清楚两者的本质和适用情况： MQTT是长连接的，本质上是发布/订阅（Pub/Sub）消息传输协议，专门设计用于低带宽、不稳定网络环境中的物联网（IoT）应用。它基于TCP/IP协议，支持高效的消息传递。 MQTT通过持久连接（保持连接）进行通信。客户端与消息代理建立长连接，在整个会话过程中保持连接，这样可以在连接断开时恢复并继续接收消息。MQTT为了减少带宽消耗，MQTT的消息头非常小，适合低带宽、高延迟的网络环境。它支持消息的质量服务（QoS）级别，确保消息传递的可靠性。 HTTP是一种请求/响应（Request/Response）协议，主要用于Web通信，基于客户端-服务器模型。HTTP通常用于浏览器与服务器之间的数据交换。HTTP每次请求都会建立一个新的连接（虽然现代HTTP（如HTTP/2）支持连接复用）。HTTP的连接通常在请求完成后关闭。HTTP的消息头较大，适合传输结构化的数据（如HTML、JSON等），但在低带宽环境下可能表现不佳。 在本次项目里面，采集前端和后端的数据传输使用HTTP而不是MQTT，因为我们需要对数据进行打包处理，即把一次(读一段时间的数据)打包发送到后端，这样处理，方便后续的数据处理与分类。处理的数据以JSON格式发送，更适合HTTP的方式。同时具有更强的鲁棒性，可以自行控制采集的时间以及发送的时间，而且采用HTTP的请求/响应模式，后端可以给采集前端发送是否接受到数据的信息，即采集前端可以知道数据传输情况，以便数据丢失的情况下可以在采集前端进行再次发送。 同时因为MQTT只是发布/订阅模式，这样采集前端就只是发布数据，至于后端的订阅有没有出问题，采集前端不好直接判断出来。同时考虑到MQTT具有较强的实时性，在此处也并不需要，因为只需要保证数据采集回来就行，所以采用MQTT会增加网络处理的压力。","link":"/2025/05/19/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E9%97%AE%E9%A2%98/"},{"title":"Git 本地版本管理操作教程","text":"本文介绍 Git 的基础安装与常见本地版本管理操作流程，包括克隆仓库、提交、分支管理等。 1. 安装 Git下载安装 Git 官网 提供的安装包，按提示安装。 2. 配置 Git 用户信息git config –global user.name “你的用户名”git config –global user.email “你的邮箱” 3、创建或者克隆仓库用于保存版本克隆仓库mkdir my_projectcd my_projectgit init 保存版本 git clone https://github.com//username/project.git 4、把需要进行版本管理的文件添加到仓库 git add filename git add . 5、提交更改 git commit -m &quot;commit message&quot; 6、查看状态和提交历史 git status (查看提交状态) git log（查看提交历史） 7、分支管理 git branch new-branch-name git checkout new-branch-name(切换到指定分支) git checkout main(回到main分支) git merge new-branch-name(合并分支) 8、添加远程仓库git remote add origin https://github.com/username/rep.git git push origin main(推送更改到远程仓库) 9、回溯版本 git log git check abc1234(找到回溯ID) git checkout main(回到最新状态) 10、其余操作 git diff(查看未提交的更改) git branch new-feature(开发分支) 通过以上操作，可以在本地进行版本管理，随时进行代码的回溯与更新！","link":"/2025/05/18/git%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/"},{"title":"Cuda基础操作","text":"本文介绍GPU结构以及Cuda编程。 1、SIMT SIMD等结构SIMD：单指令多数据 单指令同时操作多个数据元素，数据被打包到宽寄存器中，所有数据执行相同的操作。 硬件依赖：由 CPU 的向量寄存器（如 SSE、AVX）实现。 适用场景：规则数据并行任务（如数组运算、图像滤波）。 工作流程示例：4个浮点数加法（使用128位SIMD寄存器） 数据加载： 将4个单精度浮点数（32位）从内存加载到SIMD寄存器（如 XMM0 和 XMM1）。 例如：XMM0 = [A, B, C, D]，XMM1 = [E, F, G, H]。 指令执行： 单条指令 ADDPS XMM0, XMM1 执行四个浮点数的并行加法： 1XMM0 = [A+E, B+F, C+G, D+H] 结果存储： 将 XMM0 中的结果写回内存，完成4个浮点数的加法。 关键特性 严格同步：所有数据元素必须同时执行相同的操作，不支持条件分支。 高效但局限：适合规则计算，但无法处理线程间独立逻辑。 SIMT：单指令多线程 核心特点 单指令控制多个线程，每个线程独立处理数据，允许条件分支（但分支分歧会降低性能）。 硬件依赖：由 GPU 的流多处理器（SM）实现。 适用场景：复杂并行任务（如图形渲染、深度学习）。 工作流程示例：GPU处理像素着色器（32线程的线程束） 线程分组： 将32个线程打包为一个线程束（Warp），每个线程处理一个像素的着色计算。 指令发射： 线程束中的所有线程同时接收同一指令，例如： 1234567if (pixel.r &gt; 0.5) { pixel.r = 1.0; // 路径1} else { pixel.r = 0.0; // 路径2} 分支处理： 分支分歧：假设16个线程满足条件（路径1），另外16个不满足（路径2）。 串行执行： GPU先为路径1的16个线程执行 pixel.r = 1.0，路径2的线程被暂停（掩码禁用）。 再为路径2的16个线程执行 pixel.r = 0.0，路径1的线程被暂停。 结果合并： 所有线程完成分支后，继续执行后续指令（如写入显存）。 关键特性 动态分支：允许线程独立执行不同逻辑，但分歧会导致性能损失。 高吞吐量：通过大量线程掩盖内存延迟，适合大规模并行任务。 2、 SIMD vs SIMT 的对比实例场景：对数组元素执行条件加法 任务：对数组 arr 中的每个元素，若大于阈值 T，则加1，否则减1。 SIMD 的实现（假设4元素并行） 加载数据：将4个元素加载到寄存器 V0 = [a, b, c, d]。 条件判断： SIMD无法直接处理条件分支，需通过向量化条件掩码实现： 生成掩码 MASK = [a&gt;T, b&gt;T, c&gt;T, d&gt;T]。 并行计算： 使用掩码选择加1或减1： 123V1 = V0 + 1; // 所有元素加1V2 = V0 - 1; // 所有元素减1结果 = 混合(V1, V2, MASK); // 根据掩码选择结果 写入内存：将结果写回数组。 局限：所有分支路径必须预先计算，无法跳过无效计算。 SIMT 的实现（每个线程处理一个元素） 线程分配：每个线程处理一个元素（如线程0处理 arr[0]）。 条件判断： 线程独立执行： 12345if (arr[i] &gt; T) { arr[i] += 1; // 路径1} else { arr[i] -= 1; // 路径2} 分支执行： 若线程束中部分线程走路径1，部分走路径2，GPU会串行执行两段代码。 结果写入：所有线程独立更新数组元素。 优势：逻辑直观，适合复杂条件；劣势：分支分歧时性能下降。 3、CPU和GPU结构一、CPU（中央处理器）1. 设计目标CPU 是计算机的“大脑”，注重低延迟和通用性，擅长处理复杂逻辑、分支预测、顺序任务。例如：操作系统调度、应用程序逻辑、数据库查询等。 2. 核心结构 控制单元（Control Unit）负责指令解码、分支预测（如流水线技术），确保指令按顺序或乱序执行。 算术逻辑单元（ALU）执行整数、浮点运算，现代CPU通常包含多个ALU以支持并行指令（如超标量架构）。 缓存系统（Cache Hierarchy） L1缓存（分指令/数据缓存）：极低延迟，约4-64KB。 L2缓存：较大（256KB-2MB），平衡速度与容量。 L3缓存（共享缓存）：多核共享，容量可达数十MB。 寄存器直接与ALU交互的极速存储单元（如x86的RAX、ARM的X0寄存器）。 内存控制器管理对系统内存（如DDR4/DDR5）的访问，延迟低但带宽有限（约50GB/s）。 3. 典型特征 核心数量少：消费级CPU通常4-16核。 高时钟频率：3-5 GHz，通过超线程（Hyper-Threading）提升线程利用率。 复杂分支预测：通过预测执行（Speculative Execution）减少流水线停顿。 二、GPU（图形处理器）1. 设计目标GPU 专为高吞吐量并行计算优化，适合处理大量相似任务（如像素渲染、矩阵运算）。典型应用：游戏渲染、深度学习训练、科学模拟。 2. 核心结构 流多处理器（Streaming Multiprocessor, SM） CUDA核心（NVIDIA）/ 流处理器（AMD）：每个SM包含数十至数百个计算单元。 Warp调度器：管理线程束（Warp），以SIMT（单指令多线程）模式执行。 共享内存（Shared Memory）：SM内高速缓存（64KB-192KB），用于线程间通信。 全局显存（VRAM）GDDR6/HBM2显存，带宽高达1TB/s（如NVIDIA RTX 4090带宽为1TB/s）。 特殊功能单元RT Core（光线追踪）、Tensor Core（AI加速）等专用硬件。 3. 典型特征 海量核心：NVIDIA A100 GPU含6912 CUDA核心。 高内存带宽：显存带宽是CPU的10-20倍。 SIMD/SIMT并行：单指令同时操作多个数据（如32线程为一个Warp执行相同指令）。 下图所示： 为CPU和GPU两者的结构。 上图中： 绿色代表的是computational units(可计算单元) 或者称之为 cores(核心)， 橙色代表memories（内存） ， 黄色代表的是control units（控制单元）。 计算单元（cores）：由图可以知道，CPU的计算单元是“大”而“少”的，然而GPU的计算单元是“小”而“多”的。 GPU的微观物理结构： NVidia Tesla架构 拥有7组TPC（Texture/Processor Cluster，纹理处理簇） 每个TPC有两组SM（Stream Multiprocessor，流多处理器） 每个SM包含： 6个SP（Streaming Processor，流处理器） 2个SFU（Special Function Unit，特殊函数单元） L1缓存、MT Issue（多线程指令获取）、C-Cache（常量缓存）、共享内存 除了TPC核心单元，还有与显存、CPU、系统内存交互的各种部件。 仅仅展示一种结构的GPU，其余大同小异。GPU架构的共性： GPC TPC Thread SM、SMX、SMM Warp SP Core ALU FPU SFU ROP Load/Store Unit L1 Cache L2 Cache Memory Register File GPU工作机制：从Fermi开始NVIDIA使用类似的原理架构，使用一个Giga Thread Engine来管理所有正在进行的工作，GPU被划分成多个GPCs(Graphics Processing Cluster)，每个GPC拥有多个SM（SMX、SMM）和一个光栅化引擎(Raster Engine)，它们其中有很多的连接，最显著的是Crossbar，它可以连接GPCs和其它功能性模块（例如ROP或其他子系统）。 程序员编写的shader是在SM上完成的。每个SM包含许多为线程执行数学运算的Core（核心）。例如，一个线程可以是顶点或像素着色器调用。这些Core和其它单元由Warp Scheduler驱动，Warp Scheduler管理一组32个线程作为Warp（线程束）并将要执行的指令移交给Dispatch Units。 GPU中实际有多少这些单元（每个GPC有多少个SM，多少个GPC ……）取决于芯片配置本身。例如，GM204有4个GPC，每个GPC有4个SM，但Tegra X1有1个GPC和2个SM，它们均采用Maxwell设计。SM设计本身（内核数量，指令单位，调度程序……）也随着时间的推移而发生变化，并帮助使芯片变得如此高效，可以从高端台式机扩展到笔记本电脑移动。 如上图，对于某些GPU（如Fermi部分型号）的单个SM，包含： 32个运算核心 （Core，也叫流处理器Stream Processor） 16个LD/ST（load/store）模块来加载和存储数据 4个SFU（Special function units）执行特殊数学运算（sin、cos、log等） 128KB寄存器（Register File） 64KB L1缓存 全局内存缓存（Uniform Cache） 纹理读取单元 纹理缓存（Texture Cache） PolyMorph Engine：多边形引擎负责属性装配（attribute Setup）、顶点拉取(VertexFetch)、曲面细分、栅格化（这个模块可以理解专门处理顶点相关的东西）。 2个Warp Schedulers：这个模块负责warp调度，一个warp由32个线程组成，warp调度器的指令通过Dispatch Units送到Core执行。 指令缓存（Instruction Cache） 内部链接网络（Interconnect Network） GPU资源机制：1、内存架构： 部分架构的GPU与CPU类似，也有多级缓存结构：寄存器、L1缓存、L2缓存、GPU显存、系统显存。 GPU Context和延迟：由于SIMT技术的引入，导致很多同一个SM内的很多Core并不是独立的，当它们当中有部分Core需要访问到纹理、常量缓存和全局内存时，就会导致非常大的卡顿（Stall）。 例如下图中，有4组上下文（Context），它们共用同一组运算单元ALU。 假设第一组Context需要访问缓存或内存，会导致2~3个周期的延迟，此时调度器会激活第二组Context以利用ALU： 当第二组Context访问缓存或内存又卡住，会依次激活第三、第四组Context，直到第一组Context恢复运行或所有都被激活： 延迟的后果是每组Context的总体执行时间被拉长了： 但是，越多Context可用就越可以提升运算单元的吞吐量，比如下图的18组Context的架构可以最大化地提升吞吐量： ![]https://cdn.jsdelivr.net/gh/GaryAacm/image-hosting@main/Cuda/5.png() CPU-GPU异构系统：根据CPU和GPU是否共享内存，可分为两种类型的CPU-GPU架构： 上图左是分离式架构，CPU和GPU各自有独立的缓存和内存，它们通过PCI-e等总线通讯。这种结构的缺点在于 PCI-e 相对于两者具有低带宽和高延迟，数据的传输成了其中的性能瓶颈。目前使用非常广泛，如PC、智能手机等。 上图右是耦合式架构，CPU 和 GPU 共享内存和缓存。AMD 的 APU 采用的就是这种结构，目前主要使用在游戏主机中，如 PS4。 在存储管理方面，分离式结构中 CPU 和 GPU 各自拥有独立的内存，两者共享一套虚拟地址空间，必要时会进行内存拷贝。对于耦合式结构，GPU 没有独立的内存，与 GPU 共享系统内存，由 MMU 进行存储管理。 CPU-GPU数据流下图是分离式架构的CPU-GPU的数据流程图： 1、将主存的处理数据复制到显存中。 2、CPU指令驱动GPU。 3、GPU中的每个运算单元并行处理。此步会从显存存取数据。 4、GPU将显存结果传回主存。 显像机制： 水平和垂直同步信号 在早期的CRT显示器，电子枪从上到下逐行扫描，扫描完成后显示器就呈现一帧画面。然后电子枪回到初始位置进行下一次扫描。为了同步显示器的显示过程和系统的视频控制器，显示器会用硬件时钟产生一系列的定时信号。 当电子枪换行进行扫描时，显示器会发出一个水平同步信号（horizonal synchronization），简称 HSync 当一帧画面绘制完成后，电子枪回复到原位，准备画下一帧前，显示器会发出一个垂直同步信号（vertical synchronization），简称 VSync。 显示器通常以固定频率进行刷新，这个刷新率就是 VSync 信号产生的频率。虽然现在的显示器基本都是液晶显示屏了，但其原理基本一致。 CPU将计算好显示内容提交至 GPU，GPU 渲染完成后将渲染结果存入帧缓冲区，视频控制器会按照 VSync 信号逐帧读取帧缓冲区的数据，经过数据转换后最终由显示器进行显示。 双缓冲 在单缓冲下，帧缓冲区的读取和刷新都都会有比较大的效率问题，经常会出现相互等待的情况，导致帧率下降。 为了解决效率问题，GPU 通常会引入两个缓冲区，即 双缓冲机制。在这种情况下，GPU 会预先渲染一帧放入一个缓冲区中，用于视频控制器的读取。当下一帧渲染完毕后，GPU 会直接把视频控制器的指针指向第二个缓冲器。 垂直同步 双缓冲虽然能解决效率问题，但会引入一个新的问题。当视频控制器还未读取完成时，即屏幕内容刚显示一半时，GPU 将新的一帧内容提交到帧缓冲区并把两个缓冲区进行交换后，视频控制器就会把新的一帧数据的下半段显示到屏幕上，造成画面撕裂现象： 为了解决这个问题，GPU 通常有一个机制叫做垂直同步（简写也是V-Sync），当开启垂直同步后，GPU 会等待显示器的 VSync 信号发出后，才进行新的一帧渲染和缓冲区更新。这样能解决画面撕裂现象，也增加了画面流畅度，但需要消费更多的计算资源，也会带来部分延迟。 4、cuda c++编程cuda c++博客 cuda官方技术手册 详细内容看官方技术手册https://face2ai.com/CUDA-F-1-1-%E5%BC%82%E6%9E%84%E8%AE%A1%E7%AE%97-CUDA/ 更具体可以查看：博客 6、cuda编程常用以及技巧1、同步 __syncthread(); 这个函数完成，这个函数只能同步同一个块内的线程，不能同步不同块内的线程，想要同步不同块内的线程，就只能让核函数执行完成，控制程序交换主机，这种方式来同步所有线程。 内存竞争是非常危险的，一定要非常小心，这里经常出错。 2、错误处理 使用实例： 在CUDA API调用处使用 12CHECK(cudaMalloc(&amp;d_data, size)); // 检查内存分配CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice)); // 检查内存拷贝 2.在核函数调用后使用 123myKernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(params);CHECK(cudaGetLastError()); // 检查核函数启动错误CHECK(cudaDeviceSynchronize()); // 检查核函数执行错误 3、计算grid的方法 在CUDA编程中，计算网格（grid）尺寸的方式是为了确保所有数据元素都能被线程覆盖，即使数据维度无法被块（block）维度整除。代码中grid的计算公式为： 12Cppdim3 grid((nx-1)/block.x+1, (ny-1)/block.y+1); 这种计算方式的原因： 向上取整的除法： - **公式**：`(n-1)/block + 1` 等价于数学上的 **向上取整**（`ceil(n / block)`）。 - **目的**：确保当数据维度（如`nx`或`ny`）无法被块维度（如`block.x`或`block.y`）整除时，仍然分配足够的块来处理所有元素。 覆盖所有数据点： - 每个块的线程数固定（例如`block(4,2)`），但数据维度可能无法整除块维度。例如： - 若`nx=7`，`block.x=4`，直接除法`7/4=1.75`，但需要2个块才能覆盖所有7个元素。 - 使用`(7-1)/4 +1 = 1 +1 = 2`，确保生成2个块，第二个块处理剩余元素（虽然部分线程可能超出数据范围，但需在核函数中检查边界）。 4、关于GPU的模型框架 https://face2ai.com/CUDA-F-3-1-CUDA%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/ SM则包括下面这些资源： 执行单元（CUDA核） 调度线程束的调度器和调度单元 共享内存，寄存器文件和一级缓存 一、SM的硬件组成 CUDA核心分组 16个CUDA核心为一组：每个SM内部将CUDA核心（即流处理器）按功能或指令类型分组。例如，在Fermi架构中，每个SM包含32个CUDA核心，分为两组，每组16个，分别处理整数和浮点运算。 这种分组允许SM同时执行不同类型的指令（如ALU和FPU操作） 核心类型分工：整数核心处理逻辑运算和地址计算，浮点核心专注于浮点运算，例如在矩阵乘法或物理模拟中。 辅助功能单元 16个加载/存储单元：每个SM配备多个独立的内存访问单元，用于处理线程的全局内存读写请求。例如，Fermi架构的SM有16个加载/存储单元，允许每个时钟周期处理16个线程的内存操作 4个特殊功能单元（SFU）：SFU负责执行复杂数学运算（如三角函数、指数函数）和原子操作。这些单元与CUDA核心并行工作，加速特定计算场景 二、线程块与线程束的执行逻辑 线程块分配到SM后的处理 线程块分割为线程束（Warp）：线程块被分配到SM后，会被自动划分为多个线程束（Warp），每个Warp包含32个线程。例如，一个包含256线程的块会被分为8个Warp Warp调度的必要性：SM的硬件资源（如寄存器、共享内存）有限，无法同时执行所有线程。通过Warp调度器管理多个Warp的交替执行，可隐藏内存访问延迟，提高利用率 线程束的交替执行机制 零开销上下文切换：当某个Warp因等待内存访问或分支分歧暂停时，调度器立即切换至其他就绪的Warp，无需保存/恢复寄存器状态，实现无缝切换 SIMT执行模型：同一Warp内的32个线程必须执行相同的指令流。若线程因条件分支出现分歧（如if-else），SM会串行执行各分支路径，导致性能下降 。例如，若半数线程进入if分支，SM需分两次执行该Warp 三、硬件与软件的协同优化 资源分配影响性能 寄存器与共享内存竞争：每个Warp需分配独立寄存器，而共享内存由块内所有线程共享。若块内线程过多，可能导致资源不足，限制SM同时驻留的Warp数量 Bank冲突避免：共享内存分为32个Bank，若同一Warp内多个线程访问同一Bank的不同地址，会导致串行化。编程时需通过内存布局优化（如矩阵转置填充）避免冲突 实际应用中的权衡 块大小设计：块的线程数通常设为32的倍数（如128或256），以适配Warp调度机制，减少资源碎片 动态并行与同步：CUDA提供__syncthreads()实现块内同步，但块间无法直接同步，需通过多次内核启动或原子操作协调 在CUDA架构中，CUDA核心（Streaming Processor, SP）与线程块（Thread Block）是硬件与编程模型的两个关键层级，其关系可从以下角度解析： 一、物理与逻辑的对应关系 CUDA核心的硬件定位 CUDA核心是GPU的物理计算单元，每个核心负责执行单一线程的算术逻辑运算（ALU/FPU）。例如，在Ampere架构的A100 GPU中，每个SM（流多处理器）包含64个CUDA核心。 并行计算能力：单个CUDA核心无法直接对应线程块，但多个CUDA核心通过SIMT（单指令多线程）架构并行执行线程束（Warp）中的指令。 线程块的编程模型角色 线程块是逻辑上的线程集合，由程序员定义。每个线程块包含多个线程（如128-1024个），并被分配到一个SM上执行。 资源分配：线程块占用SM的共享内存和寄存器资源，例如一个SM最多同时驻留32个线程块（A100 GPU）。 二、执行机制中的交互 线程块到CUDA核心的映射 线程束（Warp）划分：线程块被分割为多个32线程的Warp，由SM的Warp调度器分配给CUDA核心执行。例如，一个包含256线程的块会被分为8个Warp。 并行执行：SM中的CUDA核心通过时间片轮转（如Fermi架构每个SM支持48个Warp并发）隐藏指令延迟，实现高效并行。 资源竞争与性能优化 寄存器与共享内存限制：每个线程块需分配独立寄存器和共享内存。若线程块过大（如每个线程使用过多寄存器），会限制SM同时处理的块数量，导致资源利用率下降 最佳实践：建议线程块大小为32的倍数（适配Warp机制），例如128或256线程/块，以平衡并行度和资源占用 5、CPU的优势 **当我们的程序包含大量的分支判断时，从程序角度来说，程序的逻辑是很复杂的，因为一个分支就会有两条路可以走，如果有10个分支，那么一共有1024条路走，CPU采用流水线话作业，如果每次等到分支执行完再执行下面的指令会造成很大的延迟，所以现在处理器都采用分支预测技术，而CPU的这项技术相对于gpu来说高级了不止一点点，而这也是GPU与CPU的不同，设计初衷就是为了解决不同的问题。CPU适合逻辑复杂计算量不大的程序，比如操作系统，控制系统，GPU适合大量计算简单逻辑的任务，所以被用来算数。** 6、分支预测当一个线程束的32个线程执行这段代码的时候，如果其中16个执行if中的代码段，而另外16个执行else中的代码块，同一个线程束中的线程，执行不同的指令，这叫做线程束的分化。我们知道在每个指令周期，线程束中的所有线程执行相同的指令，但是线程束又是分化的，所以这似乎是相悖的，但是事实上这两个可以不矛盾。解决矛盾的办法就是每个线程都执行所有的if和else部分，当一部分con成立的时候，执行if块内的代码，有一部分线程con不成立，那么他们怎么办？继续执行else？不可能的，因为分配命令的调度器就一个，所以这些con不成立的线程等待，就像分水果，你不爱吃，那你就只能看着别人吃，等大家都吃完了，再进行下一轮（也就是下一个指令）线程束分化会产生严重的性能下降。条件分支越多，并行性削弱越严重。注意线程束分化研究的是一个线程束中的线程，不同线程束中的分支互不影响。 因为线程束分化导致的性能下降就应该用线程束的方法解决，根本思路是避免同一个线程束内的线程分化，而让我们能控制线程束内线程行为的原因是线程块中线程分配到线程束是有规律的而不是随机的。这就使得我们根据线程编号来设计分支是可以的，补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。线程束内的线程是可以被我们控制的，那么我们就把都执行if的线程塞到一个线程束中，或者让一个线程束中的线程都执行if，另外线程都执行else的这种方式可以将效率提高很多。 7、关于内存分配 8、关于充分利用线程以及避免分支分化deepseek 博客 通过重新映射，使得一个线程束里面的全部线程全部活跃，另外一个线程束不工作，提高效率。 9、cuda性能分析使用nsys操作 1. 基础用法1.1 生成性能分析报告1nsys profile [选项] ./your_cuda_program 默认输出：生成 report.qdrep（二进制文件）和 report.sqlite（数据库文件）。 查看概要统计： 1nsys stats report.qdrep # 显示关键指标汇总 1.2 常用选项 选项 说明 -o &lt;文件名&gt; 指定输出文件名（默认 report） --stats=true 输出统计摘要（类似旧版 nvprof） --trace=cuda,osrt 跟踪 CUDA API 和操作系统事件 --force-overwrite=true 覆盖已有报告文件 2. 跟踪特定事件2.1 跟踪范围控制 跟踪 CUDA 活动： 1nsys profile --trace=cuda ./your_program 跟踪 CPU 系统调用： 1nsys profile --trace=osrt ./your_program # osrt = OS Runtime 多类事件组合： 1nsys profile --trace=cuda,cublas,nvtx ./your_program 2.2 时间范围限定分析程序特定阶段的性能： 1nsys profile --capture-range=cudaProfilerApi --capture-range-end=stop ./your_program 需在代码中添加范围标记： 123cudaProfilerStart(); // 开始记录// 目标代码段cudaProfilerStop(); // 结束记录 3. 硬件计数器与指标3.1 收集 GPU 硬件指标1nsys profile --gpu-metrics=device=0,metric1,metric2 ./your_program 示例（收集 SM 效率和内存带宽）： 1nsys profile --gpu-metrics=device=0,sm__throughput.avg.pct_of_peak_sustained,smsp__cycles_active.avg.pct_of_peak_sustained ./your_program 查看支持的指标： 1nsys profile --list-gpu-metrics 3.2 收集 CPU 性能数据123nsys profile --sample=cpu ./your_program # 结合 CPU 采样和 GPU 跟踪nsys profile --trace=cuda --sample=cpu ./your_program 4. 多进程与多 GPU 分析4.1 分析 MPI 多进程程序1nsys launch mpirun -n 4 ./your_mpi_program 生成单独的报告文件（每个进程一个报告）。 4.2 多 GPU 设备分析1nsys profile --cuda-device-range=0-3 ./your_multi_gpu_program --cuda-device-range=0,2：仅分析 GPU 0 和 2。 5. 高级分析功能5.1 统一内存分析跟踪统一内存（Unified Memory）的页迁移： 1nsys profile --unified-memory-profiling=per-process-device ./your_program 5.2 时间线可视化分析生成时间线数据并用 GUI 查看： 12nsys profile -o timeline ./your_programnsight-sys timeline.qdrep GUI 功能： 查看 CPU/GPU 并行时间线 分析内核与内存传输的重叠情况 标记关键区域（需代码中添加 NVTX 标记） 5.3 添加 NVTX 标记在代码中插入自定义标记： 12345#include &lt;nvtx3/nvToolsExt.h&gt;nvtxRangePushA(&quot;Data Preparation&quot;); // 开始标记// 数据准备代码nvtxRangePop(); // 结束标记 生成带 NVTX 的报告： 1nsys profile --trace=nvtx ./your_program 6. 导出与二次分析6.1 导出为 CSV1nsys export --type=csv timeline.qdrep -o timeline.csv 导出的事件包括 CUDA API、内核、内存操作等。 6.2 使用 Python 分析通过 nsys 的 Python API 解析 .sqlite 文件： 12345import sqlite3conn = sqlite3.connect('report.sqlite')cursor = conn.execute(&quot;SELECT * FROM CUPTI_ACTIVITY_KIND_KERNEL&quot;)for row in cursor.fetchall(): print(row) # 输出内核执行详情 7. 实战示例7.1 分析矩阵乘法性能12345nsys profile \\ --trace=cuda,nvtx \\ --gpu-metrics=device=0,sm__throughput.avg.pct_of_peak_sustained \\ -o matmul_profile \\ ./matmul 分析内容： 内核执行时间线 SM 计算单元利用率 自定义 NVTX 标记区域 7.2 优化内存传输1nsys profile --trace=cuda --stats=true ./memory_bound_program 检查输出中的 cuda_gpu_mem_time_sum，观察： HtoD（主机到设备）和 DtoH（设备到主机）耗时 内存带宽利用率 8、一级缓存编译在CUDA编译时通过-Xptxas -dlcm=cg启用一级缓存控制，具体命令如下： 1nvcc -arch=sm_75 -Xptxas -dlcm=cg your_code.cu -o your_program 分步解释： 架构指定： 1-arch=sm_75 # 指定Turing架构（根据实际GPU调整） 必须与GPU计算能力匹配（如RTX 2080用sm_75，A100用sm_80） 缓存控制选项： 1-Xptxas -dlcm=cg # 加载缓存模式设置 dlcm = Device Load Cache Mode cg = Cache Global（全局内存访问使用L1缓存） 完整编译流程： 12345# 带L1缓存的编译nvcc -O3 -arch=sm_75 -Xptxas -dlcm=cg main.cu -o aligned_cached# 禁用L1缓存的对比编译nvcc -O3 -arch=sm_75 -Xptxas -dlcm=ca main.cu -o aligned_nocache 关键参数说明： 选项 作用 适用场景 -dlcm=ca 强制所有加载通过L1缓存 有数据局部性的访问模式 -dlcm=cg 绕过L1直接使用L2缓存 无重用的大步长访问 -dlcm=cs 流式加载（自动缓存策略） 默认模式 验证方法： 查看PTX汇编确认效果： 1nvcc -arch=sm_75 --ptxas-options=-v -Xptxas -dlcm=cg -ptx main.cu 输出中观察： 1lca::CG // 表示使用全局缓存模式 性能对比测试： 123./aligned_cached 0 # 对齐访问+L1缓存./aligned_nocache 0 # 对齐访问+无L1缓存./aligned_cached 1 # 非对齐访问+L1缓存 典型性能差异（RTX 3080测试）： 访问模式 L1缓存 带宽利用率 执行时间 对齐 启用 92% 1.2ms 对齐 禁用 88% 1.3ms 非对齐 启用 68% 2.1ms 非对齐 禁用 72% 1.9ms","link":"/2025/05/19/cuda%E5%9F%BA%E7%A1%80/"},{"title":"我的第一篇博客","text":"欢迎来到我的博客！这是我用 Hexo 写的第一篇文章。 关于我我是一名热爱技术的开发者，记录分享是我的习惯。 由于开发技术不胜枚举，因此，作为一名想要不断提升自身的开发者，很有必要记录每一次技术操作以及出现的bug 网上也查找了很多记录blog的办法，感觉依旧是用Hexo与GitHub相结合的方式更加高大上一点点。本人目前最擅长的编程语言是C++，也希望以后可以从事相关C++的开发工作，找到一份很不错的薪资，同时对Python也有所了解，熟悉一些基本的机器学习和深度学习算法。考虑到工作的原因，可能以后还会学习Java。 目前此blog记录的是我的求学生涯之中，做过的所有的开发项目上面遇到的问题以及最后的解决办法。","link":"/2025/05/15/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"},{"title":"卷积神经网络(CNN)","text":"本文主要介绍了卷积神经网络的原理 什么是 CNN？卷积神经网络（Convolutional Neural Network, CNN） 是深度学习中处理图像数据的核心结构，它利用“局部感知 + 权重共享”的机制，有效提取图像中的空间特征，被广泛应用于图像分类、目标检测、图像分割、人脸识别等任务中。 CNN 的基本结构CNN 通常由以下几类层堆叠而成： 卷积层（Convolutional Layer） 激活函数（Activation Function, 如 ReLU） 池化层（Pooling Layer） 全连接层（Fully Connected Layer） 输出层（用于分类或回归） 1. 卷积层（Conv）卷积操作的本质是：使用小窗口滑动输入图像，对每个局部区域计算加权求和。 数学表达为： $$y(i, j) = \\sum_m \\sum_n x(i+m, j+n) \\cdot k(m, n)$$ 其中： ( x )：输入图像区域 ( k )：卷积核（滤波器） ( y(i, j) )：输出特征图上的一个像素 卷积层的作用是从图像中提取边缘、纹理、形状等局部特征。 2. 激活函数（ReLU）每次卷积输出后，需通过非线性激活函数，使网络能学习非线性模式。 常用激活函数： ReLU：$$( f(x) = \\max(0, x) )$$ Leaky ReLU Sigmoid / Tanh（较少用于 CNN） 3. 池化层（Pooling）池化层用于降低特征图空间维度、减少计算量、增强鲁棒性，防止过拟合。 常见池化方式： 最大池化（Max Pooling）：选局部最大值 平均池化（Average Pooling）：选局部平均值 例如 $$( 2 \\times 2 ) $$ 最大池化： 12345输入：[[1, 3], [2, 4]]→ 输出：4 卷积和池化操作的区别卷积：提取特征(边缘、纹理、形状等)，构建深层语义 $$y(i, j) = \\sum_m \\sum_n x(i + m, j + n) \\cdot w(m, n) + b$$ 通过加权求和来进行计算 权重是可训练的参数 池化:压缩特征图，降低尺寸，增强模型鲁棒性，减少过拟合 通过 固定规则(对特定局部区域max()或者avg()) 来进行计算 无可训练参数 4. 全连接层（Fully Connected Layer）在卷积和池化提取出图像特征后，最终的表示会展平（flatten）成向量输入到全连接层，进行分类或回归任务。 每一个神经元与上一层所有输出相连接，类似传统神经网络的结构。 通常是网络中的“决策层”。 5. 输出层（用于分类或回归）根据任务类型，输出层设置为： 分类任务：使用 softmax 激活函数输出类别概率： $$P(y_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$ 回归任务：直接输出连续数值（不加激活函数，或用线性） CNN 的整体流程1输入图像 → 卷积层 → ReLU → 池化层 → 卷积层 → ReLU → 池化层 → Flatten → 全连接层 → Softmax 关于 CNN 的图文讲解(来自CSDN) 多组卷积核的意义：我们之所以使用多组卷积核，是因为每个卷积核只能提取图像中某一种类型的局部特征，如边缘、纹理或方向变化等，而图像中的信息是多样且复杂的。通过使用多组卷积核，模型可以从不同角度、多种尺度上提取丰富的特征，增强对图像的理解能力。同时，对于彩色图像，每组卷积核会分别作用于多个通道（如RGB），并整合它们的特征，从而构建更有表现力的特征图，为后续的分类、检测等任务提供更强的信息基础。 CNN代码展示：12345678910111213141516171819import torch.nn as nnimport torch.nn.functional as Fclass SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # 输入通道1，输出通道32 self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 卷积层2 self.pool = nn.MaxPool2d(2, 2) # 最大池化层 self.fc1 = nn.Linear(64 * 7 * 7, 128) # 全连接层1 self.fc2 = nn.Linear(128, 10) # 全连接层2，输出10类 def forward(self, x): x = self.pool(F.relu(self.conv1(x))) # Conv1 + ReLU + Pool x = self.pool(F.relu(self.conv2(x))) # Conv2 + ReLU + Pool x = x.view(-1, 64 * 7 * 7) # 展平 x = F.relu(self.fc1(x)) # 全连接1 + ReLU x = self.fc2(x) # 全连接2（输出层） return x 卷积通用计算公式设输入特征图大小为： $$H_{in} \\times W_{in}$$ 卷积核大小为 $$( K \\times K )$$ $$步长为 ( S )$$ 填充为 $$( P )$$ 则输出特征图大小为： $$H_{out} = \\left\\lfloor \\frac{H_{in} - K + 2P}{S} \\right\\rfloor + 1$$ $$W_{out} = \\left\\lfloor \\frac{W_{in} - K + 2P}{S} \\right\\rfloor + 1$$","link":"/2025/05/25/CNN%E5%9F%BA%E7%A1%80/"},{"title":"Transformer基础","text":"本文主要介绍了Transformer的原理 引言Transformer 是深度学习中的一次革命。2017 年，Google 在论文 《Attention is All You Need》 中首次提出该结构，在完全抛弃 RNN/CNN 的前提下，仅利用 注意力机制（Attention） 便刷新多项 NLP 基准任务成绩。此后，GPT、BERT、T5、ViT 等明星模型无一不是 Transformer 的变体或衍生。 一文读懂Transformer(CSDN转载) 为什么要用 Transformer？ 传统序列模型痛点 Transformer 优势 序列计算串行：RNN/LSTM 依赖前一步输出，无法并行 完全并行：自注意力一次性建模所有位置 长距离依赖困难：梯度消失 / 爆炸 全局依赖：任意两位置直接交互 训练效率低：序列越长越慢 收敛更快：GPU/TPU 高效并行矩阵乘 表达能力有限：难以捕捉复杂模式 扩展性强：易于叠加深层、跨模态 Transformer 模块结构总览Transformer 架构可分为以下模块： 位置编码（Positional Encoding） 多头注意力机制（Multi-Head Attention） 残差连接与层归一化（Residual + LayerNorm） 每个 Encoder Block 就是这些模块的组合： 1234567891011121314Input Embedding ↓Positional Encoding ↓Multi-Head Self-Attention ↓Residual + LayerNorm ↓Feed Forward Network ↓Residual + LayerNorm ↓Output 位置编码由于 Transformer 没有像 RNN 那样的时间结构，它无法天然获取词语的顺序信息。因此，需要通过位置编码（Positional Encoding）来引入序列的位置信息。 原论文提出了基于三角函数的固定位置编码，数学表达如下： $$\\begin{aligned}PE_{(pos, 2i)} &amp;= \\sin\\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right) \\PE_{(pos, 2i+1)} &amp;= \\cos\\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right)\\end{aligned}$$ ( pos )：词在序列中的位置 ( i )：维度索引 $$( d_{\\text{model}} )$$ 词向量的维度大小 特点： 可推广到任意序列长度 不增加训练参数 不同维度周期不同，可表示位置相对关系 现代 Transformer 也常使用可学习的位置编码，或如 RoPE、ALiBi 等改进形式。 多头注意力机制注意力机制的目标是：让每个词动态关注输入序列中的其他词，捕捉上下文依赖关系。 基本注意力机制（Scaled Dot-Product Attention）如下： $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V$$ ( Q )（Query）：当前词向量 ( K )、( V )：输入序列中所有词作为键和值 $$( d_k )$$ 缩放因子，防止梯度过小 多头注意力（Multi-Head Attention）将输入拆分成多个子空间并行执行注意力，再拼接输出： $$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, …, \\text{head}_h)W^O$$ 每个 head 可以学习不同类型的依赖，比如： 局部 vs 全局语义 位置对齐 vs 语法关系 残差连接与层归一化Transformer 堆叠多个子模块，为了避免梯度消失并保持信息流通，引入了残差连接（Residual Connection）与层归一化（LayerNorm）。 残差连接的表达式如下： $$\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$ ( x )：子层的输入 $$( \\text{Sublayer}(x) )$$ 如 Self-Attention 或 FFN 的输出 作用： Residual：信息跳跃传播，缓解深层退化 LayerNorm：标准化每个样本的通道，提升训练稳定性（不同于 BatchNorm） 在每个 Attention 层和 FFN 层之后都采用了这个组合。","link":"/2025/05/25/Transformer%E5%9F%BA%E7%A1%80/"},{"title":"BP神经网络","text":"本文主要介绍了BP神经网络的原理 BP神经网络 BP神经网络介绍(CSDN转载) 一、什么是BP神经网络BP（Back Propagation）神经网络是一种前馈型人工神经网络，广泛应用于模式识别、函数逼近、数据预测等领域。它通过误差反向传播算法进行训练，是多层感知机（MLP）的基础。 BP神经网络的基本结构包括输入层、隐藏层和输出层。 通过层层传递并不断调整权重，最终逼近目标输出。 二、网络结构一个典型的BP神经网络包含以下三部分： 输入层（Input Layer）：用于接收外部输入数据。 隐藏层（Hidden Layer）：执行非线性变换，层数和每层神经元数目可调。 输出层（Output Layer）：输出预测结果或分类结果。 三、工作原理BP神经网络的训练过程分为两个阶段： 1. 前向传播（Forward Propagation）输入数据从输入层经过隐藏层传递到输出层，每层的输出是上一层加权求和后的激活函数结果： $$y = f(\\sum w_ix_i + b)$$ 常用激活函数有 Sigmoid、Tanh、ReLU 等。 2. 反向传播（Back Propagation）根据输出误差，使用梯度下降法按以下步骤调整权重： 计算输出误差：\\( E = \\frac{1}{2}(y_{target} - y_{output})^2 \\) 反向传播误差：根据链式法则将误差传递至隐藏层。 更新权重和偏置：通过学习率 \\( \\eta \\) 调整网络参数。 四、BP算法流程 初始化网络权重和偏置； 输入样本，执行前向传播； 计算误差并执行反向传播； 更新权重和偏置； 重复步骤2~4，直到误差收敛或达到最大迭代次数。 五、BP神经网络的优缺点优点： 理论成熟，适用于多种非线性映射问题； 支持多层结构，具备良好的表示能力； 可通过不同激活函数适配不同任务。 缺点： 容易陷入局部最优； 训练过程耗时，收敛速度慢； 对初始权重、学习率等参数较敏感； 隐藏层层数选择缺乏理论指导。 六、应用场景 图像识别 语音处理 股票预测 医疗诊断 工业控制系统 七、总结BP神经网络作为深度学习的基础模型，其核心思想——误差反向传播算法至今仍在许多先进模型中得以继承和发展。理解BP网络的结构和工作原理，有助于深入掌握现代神经网络模型的本质。","link":"/2025/05/27/BP%E5%9F%BA%E7%A1%80/"},{"title":"ROS2 入门指南","text":"ROS2 入门指南 本文是我学习 ROS2（Robot Operating System 2）的笔记与总结，适合零基础或从 ROS1 过渡的同学。ROS2 相比 ROS1 在性能、通信、跨平台等方面都有重大改进。 1. ROS2 简介 ROS2 是什么？ 为什么选择 ROS2？ ROS1 与 ROS2 的区别 常见应用场景（自动驾驶、机器人、无人机等） 2. ROS2 安装2.1 系统要求 推荐操作系统 依赖工具（colcon、python3 等） 2.2 安装步骤（以 Ubuntu 为例）sudo apt update &amp;&amp; sudo apt upgrade sudo apt install ros-humble-desktop","link":"/2025/08/14/ROS2%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"Qt","slug":"Qt","link":"/tags/Qt/"},{"name":"qmake","slug":"qmake","link":"/tags/qmake/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"个人规划","slug":"个人规划","link":"/tags/%E4%B8%AA%E4%BA%BA%E8%A7%84%E5%88%92/"},{"name":"MAX30102","slug":"MAX30102","link":"/tags/MAX30102/"},{"name":"I2C","slug":"I2C","link":"/tags/I2C/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"嵌入式开发","slug":"嵌入式开发","link":"/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"tensorRT","slug":"tensorRT","link":"/tags/tensorRT/"},{"name":"POSIX","slug":"POSIX","link":"/tags/POSIX/"},{"name":"QThread","slug":"QThread","link":"/tags/QThread/"},{"name":"MQTT","slug":"MQTT","link":"/tags/MQTT/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"API","slug":"API","link":"/tags/API/"},{"name":"网络通信","slug":"网络通信","link":"/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"版本控制","slug":"版本控制","link":"/tags/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"},{"name":"cuda","slug":"cuda","link":"/tags/cuda/"},{"name":"C++C","slug":"C-C","link":"/tags/C-C/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"博客搭建","slug":"博客搭建","link":"/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"YOLO","slug":"YOLO","link":"/tags/YOLO/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"BP","slug":"BP","link":"/tags/BP/"},{"name":"神经网络","slug":"神经网络","link":"/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"ROS2","slug":"ROS2","link":"/tags/ROS2/"},{"name":"机器人","slug":"机器人","link":"/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/"},{"name":"入门教程","slug":"入门教程","link":"/tags/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"}],"categories":[{"name":"技术分享","slug":"技术分享","link":"/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"},{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"个人介绍","slug":"个人介绍","link":"/categories/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/"},{"name":"机器人开发","slug":"机器人开发","link":"/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E5%8F%91/"}],"pages":[]}